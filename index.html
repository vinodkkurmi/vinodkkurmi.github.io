<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html class="gr__githu_io_in_"><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Credits: Jon Barron and Abhishek Kar and Saurabh Gupta*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 19px;
    font-weight: 1000
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 800
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }
  </style>
  <link rel="icon" type="image/png" href="hhttp://home.iitk.ac.in/~vinodkk/idda_model/iitk.png">
  <title>Vinod Kumar Kurmi</title>
  <meta name="Vinod Kumar Kurmi&#39;s IITK Homepage" http-equiv="Content-Type" content="Vinod Kumar Kurmi&#39;s IITK Homepage">
  <link href="./Vinod_files/css" rel="stylesheet" type="text/css">
  <!-- Start : Google Analytics Code -->

  <!-- End : Google Analytics Code -->
  <!-- Scramble Script by Jeff Donahue -->
  <script src="./Vinod_files/scramble.js"></script>
<style type="text/css">@keyframes fadeInOpacity{0%{opacity:0}to{opacity:1}}:hover>*>.fbvd--wrapper{animation-name:fadeInOpacity;animation-duration:.3s;opacity:1}.fbvd--wrapper{position:absolute;top:10px;left:10px;opacity:0;text-align:center;margin:0;z-index:5}.fbvd--wrapper a{background: url(data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0idXRmLTgiPz48IURPQ1RZUEUgc3ZnIFBVQkxJQyAiLS8vVzNDLy9EVEQgU1ZHIDEuMS8vRU4iICJodHRwOi8vd3d3LnczLm9yZy9HcmFwaGljcy9TVkcvMS4xL0RURC9zdmcxMS5kdGQiPjxzdmcgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdCb3g9IjAgMCAxNiAxNiI+PHBhdGggZmlsbD0iIzRiNGY1NiIgZD0iTTggMTUuNWw3LjUtNy41aC00LjV2LThoLTZ2OGgtNC41eiI+PC9wYXRoPjwvc3ZnPg==) no-repeat 3px 4.55px; background-color: #fff; display:inline-block;font:700 14px Helvetica,Arial,sans-serif;color:#4b4f56;text-decoration:none;vertical-align:middle;padding:0px 8px 0px;margin-right:8px;border-radius:2px; line-height: 22px; padding-left:19px; border: 1px solid #e7e7e7; background-size: 13px}.fbvd--wrapper a:last-child{margin-right:0}.fbvd--wrapper a:hover{text-decoration:none}.fbvd--wrapper a:focus{box-shadow:0 0 1px 2px rgba(88,144,255,.75),0 1px 1px rgba(0,0,0,.15);outline:none}.fbvd--wrapper b{font-size:13px;position:relative;top:1px;color:#3b5998;font-weight:400}</style></head>

<body data-gr-c-s-loaded="true">
<table width="840" border="0" align="center" cellspacing="0" cellpadding="20">
  <tbody><tr><td>

<p align="center">
    <!-- <font size="7">Deepak Pathak</font><br> -->
    <pageheading>Vinod Kumar Kurmi</pageheading><br>
    <b>Email</b>: vinodkk@iitk.ac.in, vinodkumarkurmi@gmail.com
    <!-- <font id="email" style="display:inline;">b.eet@paykcrslkaehde.u <a href="https://vinodkkurmi.github.io" onclick="emailScramble.initAnimatedBubbleSort();return false;">unscramble</a></font>
    <script>
    emailScramble = new scrambledString(document.getElementById('email'),
        'emailScramble', 'vinodkk@iitk.ac.in',
        [13,12,15,1,8,7,2,5,4,11,10,17,3,16,6,9,14]);
    </script> -->
  </p><table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">


  <tbody><tr>
    <td width="32%" valign="top"><a href="./Vinod_files/1profile_pic.JPG"><img src="./Vinod_files/profile_pic.JPG" width="100%" style="border-radius:15px"></a>
    <p align="center">
    <a href="./Vinod_files/vinodkk_cv.pdf">CV</a> | <a href="https://scholar.google.co.in/citations?user=Exo2VNAAAAAJ&hl=en">Scholar</a> | <a href="https://github.com/vinodkkurmi">Github</a> | <a href="https://twitter.com/vinodkkurmi"> Twitter </a>
    </p>
    </td>
    <td width="68%" valign="top" align="justify">
    <p>I joined as a post-doc fellow at <a href="https://cvit.iiit.ac.in/">CVIT Hyderabad</a>, working with <a href="http://www.cse.iitk.ac.in/users/vinaypn/">Prof. C.V. Jawahar</a>. I am a <a href="https://www.daad.de/en/the-daad/postdocnet/fellows/fellows/">DAAD AI Post-Doc-Net Fellow </a> for 2020-21.
        </p>
<p>
        I obtained Ph.D. from  <a href="http://www.iitk.ac.in/">Indian Institute of Technology Kanpur</a> under the supervision of <a href="http://www.cse.iitk.ac.in/users/vinaypn/">Prof. Vinay P Namboodiri</a> and <a href="http://home.iitk.ac.in/~venkats/">Prof. K. S. Venkatesh</a>. I was associated with the <a href="http://deltalab.iitk.ac.in/">Delta Lab</a> and <a href="http://iitk.ac.in/ee/computer-vision-lab">Computer Vision Lab</a> and was a TCS-Research fellow from 2015-2020.
        </p>

<p>
    My research areas are related to Computer Vision (CV), Deep Learning (DL) and Machine Learning (ML).
 Currently I am working in Domain Adaptation and Multi-Modal Alignment Problems.  </p>


</p>





<h3>Research Interest</h3>

<ul>
  <li>Domain Adaptation</li>
  <li>MultiModal Generation </li>
  <li>Bayesian Models in Deep Learning</li>
  <li>Causality and Incremental Learning </li>

</ul>
    </td>
  </tr>
</tbody></table>





<!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tbody><tr><td>
    <sectionheading>&nbsp;&nbsp;News</sectionheading>
    <ul>
    <li> Qualified for Qualcomm Innovation Fellowship 2019 Finals.</li> -->
    <!-- <li> <a href="#ICML19">Paper</a> on real-robot exploration via disagreement accepted at ICML'19!</li> -->

    <!-- <li> <a href="#NIPS17">Paper</a> accepted at NIPS 2017 on multimodal image generation.</li> -->
    <!-- <li> <a href="#ICLR18">Paper</a> on zero-shot visual imitation accepted at <a href="https://openreview.net/group?id=ICLR.cc/2018/Conference" target="_blank">ICLR 2018</a>.</li> -->
    <!-- <li> <a href="#ICML17">Paper</a> accepted at ICML 2017 on curiosity-driven exploration for reinforcement learning. <a href="http://pathak22.github.io/noreward-rl/index.html#demoVideo">Video!</a></li> -->
    <!-- <li> <a href="#CVPR17">Paper</a> accepted at CVPR 2017 on unsupervised learning using unlabeled videos.</li> -->
    <!-- <li> <a href="#CVPR16">Paper</a> accepted at CVPR 2016 on unsupervised learning and inpainting. <a href="context_encoder/">Check out!</a></li> -->
    <!-- <li> <a href="#JMLR16">Paper</a> accepted at JMLR 2016; extension of <a href="#CVPR15">CVPR'15</a> paper.</li> -->
    <!-- <li> Paper about constrained structured regression (applied to intrinsics) on <a href="http://arxiv.org/abs/1511.07497">arXiv</a>.</li> -->
    <!-- <li> <a href="#ICCV15">Paper</a> accepted at ICCV 2015 on Constrained CNN for segmentation. Code released on <a href="http://github.com/pathak22/ccnn">github</a> !</li> -->
    <!-- <li> Undergrad <a href="#JPM15">paper</a> related to predicting <a href="http://www.huffingtonpost.co.uk/2015/02/23/microsoft-oscar-predictio_n_6735122.html">Oscars</a> published at <a href="http://ubplj.org/index.php/jpm/article/view/1048">JPM</a>. See <a href="#oscarPred">live predictions</a>.</li> -->
    <!-- <li> <a href="#CVPR15">Paper</a> accepted at CVPR 2015 on domain adaptation</li> -->
    <!-- <li> <a href="#ICLR15">Paper</a> accepted at ICLR Workshop 2015</li> -->
<!--     </ul>
  </td></tr>
</tbody></table> -->



<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
      <tbody><tr>
        <td>
        <heading>Some recent news</heading>
        </td>
      </tr>
      </tbody></table>
      <table class="news-table" width="100%" align="center" border="0" style="text-align: justify">
    <colgroup><col width="15%">
    <col width="85%">
    </colgroup><tbody>
    <tr>
        <td valign="top" align="center"><strong>Dec 2020</strong></td>
        <td>Defended PhD thesis on  "Understading Transfer Learning between Domains and Tasks".</td>
    </tr>
    <tr>

        <tr>
        <td valign="top" align="center"><strong>Nov 2020</strong></td>
        <td>Paper accepted in workshop of <a href="http://vcmi.inesctec.pt/xai4biom_wacv/index.html
">xAI4Biometrics</a> at WACV-21.</td>
    </tr>
    <tr>
        <tr>
        <td valign="top" align="center"><strong>Nov 2020</strong></td>
        <td>Two papers accepted in <a href="http://wacv2021.thecvf.com/"> WACV-21.</a> </td>
    </tr>
    <tr>

    <tr>
        <tr>
        <td valign="top" align="center"><strong>Oct 2020</strong></td>
        <td>Selected as an DAAD <a href="https://www.daad.de/en/the-daad/postdocnet/fellows/fellows/#Kurmi
"> AI Post-Doc Net Fellow</a> for </strong> (2020-2021). </td>
    </tr>
    <tr>


   <tr>
        <tr>
        <td valign="top" align="center"><strong>Aug 2020</strong></td>
        <td>Joined as a Post-Doc at <a href="https://cvit.iiit.ac.in/
">CVIT Hyderabad.</a>  </td>
    </tr>
    <tr>

            <tr>
        <tr>
        <td valign="top" align="center"><strong>July 2020</strong></td>
        <td> Paper accepted in <a href="https://www.journals.elsevier.com/neurocomputing"> Neurocomputing</a>. </td>
    </tr>
    <tr>





<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
      <tbody><tr>
        <td>
        <heading><marquee behavior="alternate" direction="right" scrolldelay="2000"><a href="https://vinodkkurmi.github.io/vinodkk_news.html"> More [Click here]</a></heading>
        </td>
      </tr>
      </tbody></table>
      <table class="news-table" width="100%" align="center" border="0" style="text-align: justify">
    <colgroup><col width="15%">
    <col width="85%">
    </colgroup><tbody>

<!--   <tr>
        <td valign="top" align="center"><strong></strong></td>
        <td><marquee behavior="alternate" direction="right" scrolldelay="200"><a href="http://wacv2020.thecvf.com/"> more details</a>.</td>
    </tr>
 -->


<!-- </tbody></table>
 <tr>
 <td valign="top" align="center"><strong>Mar 2019</strong></td> <marquee behavior="alternate" direction="right" scrolldelay="200"><a href="http://wacv2020.thecvf.com/"> more details</a>.

 </marquee>
 </tr>
 -->







<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tbody><tr><td><sectionheading>&nbsp;&nbsp;Publications</sectionheading></td></tr>
</tbody></table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tbody>


        <tr>
    <td width="33%" valign="top" align="center"><a href="https://vinodkkurmi.github.io"><img src="./Vinod_files/wacv2021_sdda.png" alt="sym" width="100%" style="border-style: none"></a>
    </td><td width="67%" valign="top">
      <p><a href="https://vinodkkurmi.github.io" id="wacv2021">
        <img src="./Vinod_files/new.png"  width="6%" style="border-style: none">
      <heading>Domain Impression: A Source Data Free Domain Adaptation Method</heading></a><br>
       <strong>Vinod K Kurmi</strong>,  Venkatesh K Subramanian,  Vinay P Namboodiri<br>
      <em>IEEE Winter Conference of Applications on Computer Vision (WACV)</em>, 2021<br>
     <!--  <em><a href="https://sites.google.com/view/cvpr2018-robotic-vision/home" style="color:black" target="_blank">Deep Learning in Robotics Vision Workshop</a> (CVPR)</em>, 2018 (Oral)<br> -->
      </p>

      <div class="paper" id="cvprw18">
      <!-- <a href="https://delta-lab-iitk.github.io/">webpage</a> | -->
      <!-- <a href="https://arxiv.org/pdf/2001.08779.pdf">pdf</a> | -->
      <a href="javascript:toggleblock(&#39;cvprw18_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;cvprw18&#39;)" class="togglebib">bibtex</a> |
      <!-- <a href="https://arxiv.org/pdf/2001.08779.pdf">arXiv</a> | -->
      <!-- <a href="https://github.com/DelTA-Lab-IITK/Incremental-learning-AU">code</a> -->
      <br>

      <p align="justify"> <i id="cvprw18_abs" style="display: none;">Unsupervised Domain adaptation methods solve the adaptation problem for an unlabeled target set, assuming that the source dataset is available with all labels. However, the availability of actual source samples is not always possible in practical cases. It could be due to memory constraints, privacy concerns, and challenges in sharing data. This practical scenario creates a bottleneck in the domain adaptation problem. This paper addresses this challenging scenario by proposing a domain adaptation technique that does not need any source data. Instead of the source data, we are only provided with a classifier that is trained on the source data. Our proposed approach is based on a generative framework, where the trained classifier is used for generating samples from the source classes. We learn the joint distribution of data by using the energy-based modeling of the trained classifier. At the same time, a new classifier is also adapted for the target domain. We perform various ablation analysis under different experimental setups and demonstrate that the proposed approach achieves better results than the baseline models in this extremely novel scenario.</i></p>

<pre xml:space="preserve" style="display: none;">@inproceedings{vinodkk2020sdda,
  title={Domain Impression: A Source Data Free Domain Adaptation Method},
  author={Kurmi, Vinod Kumar and K S , Venaktesh and  Namboodiri, Vinay},
  booktitle={IEEE Winter Conference of Applications on Computer Vision (WACV)},
<!--   pages={4002--4012}, -->
<!--   year={2018} -->
}
</pre>
      </div>
    </td>
  </tr>






     <tr>
    <td width="33%" valign="top" align="center"><a href="https://vinodkkurmi.github.io"><img src="./Vinod_files/wacv2021_incre.png" alt="sym" width="100%" style="border-style: none"></a>
    </td><td width="67%" valign="top">
      <p><a href="https://vinodkkurmi.github.io" id="wacv2021">
        <img src="./Vinod_files/new.png"  width="6%" style="border-style: none">
      <heading>Do not Forget to Attend to Uncertainty while Mitigating Catastrophic Forgetting</heading></a><br>
       <strong>Vinod K Kurmi</strong>, Badri N Patro, Venkatesh K Subramanian,  Vinay P Namboodiri<br>
      <em>IEEE Winter Conference of Applications on Computer Vision (WACV)</em>, 2021<br>
     <!--  <em><a href="https://sites.google.com/view/cvpr2018-robotic-vision/home" style="color:black" target="_blank">Deep Learning in Robotics Vision Workshop</a> (CVPR)</em>, 2018 (Oral)<br> -->
      </p>

      <div class="paper" id="cvprw18">
      <a href="https://delta-lab-iitk.github.io/Incremental-learning-AU/">webpage</a> |
      <!-- <a href="https://arxiv.org/pdf/2001.08779.pdf">pdf</a> | -->
      <a href="javascript:toggleblock(&#39;cvprw18_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;cvprw18&#39;)" class="togglebib">bibtex</a> |
      <!-- <a href="https://arxiv.org/pdf/2001.08779.pdf">arXiv</a> | -->
      <a href="https://github.com/DelTA-Lab-IITK/Incremental-learning-AU">code</a>
      <br>

      <p align="justify"> <i id="cvprw18_abs" style="display: none;">One of the major limitations of deep learning models is that they face catastrophic forgetting in an incremental learning scenario.  There have been several approaches proposed to tackle the problem of incremental learning. Most of these methods are based on knowledge distillation and do not adequately utilize the information provided by older task models, such as uncertainty estimation in predictions. The predictive uncertainty provides the distributional information can be applied to mitigate catastrophic forgetting in a deep learning framework.  In the proposed work, we consider a Bayesian formulation to obtain the data and model uncertainties. We also incorporate self-attention framework to address the incremental learning problem. We define distillation losses in terms of aleatoric uncertainty and self-attention. In the proposed work, we investigate different ablation analyses on these losses. Furthermore, we are able to obtain better results in terms of accuracy on standard benchmarks.</i></p>

<pre xml:space="preserve" style="display: none;">@inproceedings{vinodkk2020incre,
  title={Do not Forget to Attend to Uncertainty while Mitigating Catastrophic Forgetting},
  author={Kurmi, Vinod Kumar and Patro, Badri Narayana and K S , Venaktesh and  Namboodiri, Vinay},
  booktitle={IEEE Winter Conference of Applications on Computer Vision (WACV)},
<!--   pages={4002--4012}, -->
<!--   year={2018} -->
}
</pre>
      </div>
    </td>
  </tr>



     <tr>
    <td width="33%" valign="top" align="center"><a href="https://vinodkkurmi.github.io"><img src="./Vinod_files/neuro_1.png" alt="sym" width="100%" style="border-style: none"></a>
    </td><td width="67%" valign="top">
      <p><a href="https://vinodkkurmi.github.io" id="wacv2020">
<!--         <img src="./Vinod_files/new.png"  width="6%" style="border-style: none"> -->
      <heading>Revisiting Paraphrase Question Generator using Pairwise
Discriminator</heading></a><br>
       Badri N Patro, Dev Chauhan, <strong>Vinod K Kurmi</strong>,  Vinay P Namboodiri<br>
      <em>Neurocomputing </em>, 2020<br>
     <!--  <em><a href="https://sites.google.com/view/cvpr2018-robotic-vision/home" style="color:black" target="_blank">Deep Learning in Robotics Vision Workshop</a> (CVPR)</em>, 2018 (Oral)<br> -->
      </p>

      <div class="paper" id="cvprw18">
      <a href="https://arxiv.org/pdf/1912.13149.pdf">pdf</a> |
      <a href="javascript:toggleblock(&#39;cvprw18_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;cvprw18&#39;)" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/pdf/1912.13149.pdf">arXiv</a> |
      <a href="https://github.com/dev-chauhan/PQG-pytorch">code</a>
      <br>

      <p align="justify"> <i id="cvprw18_abs" style="display: none;">Generating natural questions from an image is a semantic task that requires using visual and language modality to learn multimodal representations. Images can have multiple visual and language contexts that are relevant for generating questions namely places, captions, and tags. In this paper, we propose the use of exemplars for obtaining the relevant context. We obtain this by using a Multimodal Differential Network to produce natural and engaging questions. The generated questions show a remarkable similarity to the natural questions as validated by a human study. Further, we observe that the proposed approach substantially improves over state-of-the-art benchmarks on the quantitative metrics (BLEU, METEOR, ROUGE, and CIDEr).</i></p>

<pre xml:space="preserve" style="display: none;">@inproceedings{patro2020Bayesian,
  title={Deep Bayesian Network for Visual Question Generation},
  author={Patro, Badri Narayana and Kumar, Sandeep and Kurmi, Vinod Kumar and Namboodiri, Vinay},
  booktitle={IEEE Winter Conference of Applications on Computer Vision (WACV)},

<!--   year={2018} -->
}
</pre>
      </div>
    </td>
  </tr>



     <tr>
    <td width="33%" valign="top" align="center"><a href="https://vinodkkurmi.github.io"><img src="./Vinod_files/wacv2020.png" alt="sym" width="100%" style="border-style: none"></a>
    </td><td width="67%" valign="top">
      <p><a href="https://vinodkkurmi.github.io" id="wacv2020">
<!--         <img src="./Vinod_files/new.png"  width="6%" style="border-style: none"> -->
      <heading>Deep Bayesian Network for Visual Question Generation</heading></a><br>
       Badri N Patro, <strong>Vinod K Kurmi</strong>, Sandeep Kumar,  Vinay P Namboodiri<br>
      <em>IEEE Winter Conference of Applications on Computer Vision (WACV)</em>, 2020<br>
     <!--  <em><a href="https://sites.google.com/view/cvpr2018-robotic-vision/home" style="color:black" target="_blank">Deep Learning in Robotics Vision Workshop</a> (CVPR)</em>, 2018 (Oral)<br> -->
      </p>

      <div class="paper" id="cvprw18">
      <a href="https://delta-lab-iitk.github.io/BVQG/">webpage</a> |
      <a href="https://arxiv.org/pdf/2001.08779.pdf">pdf</a> |
      <a href="javascript:toggleblock(&#39;cvprw18_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;cvprw18&#39;)" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/pdf/2001.08779.pdf">arXiv</a> |
      <a href="https://github.com/DelTA-Lab-IITK/BVQG">code</a>
      <br>

      <p align="justify"> <i id="cvprw18_abs" style="display: none;">Generating natural questions from an image is a semantic task that requires using visual and language modality to learn multimodal representations. Images can have multiple visual and language contexts that are relevant for generating questions namely places, captions, and tags. In this paper, we propose the use of exemplars for obtaining the relevant context. We obtain this by using a Multimodal Differential Network to produce natural and engaging questions. The generated questions show a remarkable similarity to the natural questions as validated by a human study. Further, we observe that the proposed approach substantially improves over state-of-the-art benchmarks on the quantitative metrics (BLEU, METEOR, ROUGE, and CIDEr).</i></p>

<pre xml:space="preserve" style="display: none;">@inproceedings{patro2020Bayesian,
  title={Deep Bayesian Network for Visual Question Generation},
  author={Patro, Badri Narayana and Kumar, Sandeep and Kurmi, Vinod Kumar and Namboodiri, Vinay},
  booktitle={IEEE Winter Conference of Applications on Computer Vision (WACV)},

<!--   year={2018} -->
}
</pre>
      </div>
    </td>
  </tr>


    <tr>
    <td width="33%" valign="top" align="center"><a href="https://delta-lab-iitk.github.io/CD3A/"><img src="./Vinod_files/bmvc19.png" alt="sym" width="100%" style="border-radius:15px"></a>
    </td><td width="67%" valign="top">
      <p><a href="https://delta-lab-iitk.github.io/CD3A/" id="bmvc19">
<!--       <img src="./Vinod_files/new.png"  width="6%" style="border-style: none"> -->
      <heading>Curriculum based Dropout Discriminator for Domain Adaptation</heading></a><br>
      <strong>Vinod K Kurmi</strong>, Vipul Bajaj, Venkatesh K Subramanian, Vinay P Namboodiri<br>
      <em>British Machine Vision Conference (BMVC) </em>, 2019<br>
      </p>

      <div class="paper" id="bmvc19">
      <a href="https://delta-lab-iitk.github.io/CD3A/">webpage</a> |
      <a href="https://arxiv.org/pdf/1907.10628.pdf">pdf</a> |
      <a href="javascript:toggleblock(&#39;bmvc19_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;bmvc19&#39;)" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/pdf/1907.10628.pdf">arXiv</a> |
      <a href="https://github.com/DelTA-Lab-IITK/CD3A">code</a>
     <!--  <a href="https://youtu.be/POlrWt32_ec">video</a> |
      <a href="https://videoken.com/embed/D0UmVbbJxS8?tocitem=89">oral talk</a> -->
      <br>

      <p align="justify"> <i id="bmvc19_abs" style="display: none;">Domain adaptation is essential to enable wide usage of deep learning based networkstrained using large labeled datasets. Adversarial learning based techniques have showntheir utility towards solving this problem using a discriminator that ensures source andtarget distributions are close. However, here we suggest that rather than using a pointestimate, it would be useful if a distribution based discriminator could be used to bridgethis gap. This could be achieved using multiple classifiers or using traditional ensemblemethods. In contrast, we suggest that a Monte Carlo dropout based ensemble discrim-inator could suffice to obtain the distribution based discriminator. Specifically, we pro-pose a curriculum based dropout discriminator that gradually increases the variance ofthe sample based distribution and the corresponding reverse gradients are used to alignthe source and target feature representations. The detailed results and thorough ablationanalysis show that our model outperforms state-of-the-art results.</i></p>

<pre xml:space="preserve" style="display: none;">@article{kurmi2019curriculum,
title={Curriculum based Dropout Discriminator for Domain Adaptation},
author={Kurmi, Vinod Kumar and Bajaj, Vipul and Subramanian, Venkatesh K and Namboodiri, Vinay P},
journal={arXiv preprint arXiv:1907.10628},
year={2019} }
</pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://vinodkkurmi.github.io/DiscriminatorDomainAdaptation/"><img src="./Vinod_files/ijcnn.png" alt="sym" width="100%" style="border-radius:15px"></a>
    </td><td width="67%" valign="top">
      <p><a href="https://vinodkkurmi.github.io/DiscriminatorDomainAdaptation/" id="ijcnn19">
<!--       <img src="./Vinod_files/new.png"  width="6%" style="border-style: none"> -->
      <heading>Looking back at Labels: A Class based Domain Adaptation Technique</a><br>
      <strong>Vinod K Kurmi</strong>, Vinay P Namboodiri<br>
      <em>International Joint Conference on Neural Networks (IJCNN)</em>, 2019
      <strong style="color:black">(Oral presentation)</strong>
      </p>

      <div class="paper" id="assemblies19">
      <a href="https://vinodkkurmi.github.io/DiscriminatorDomainAdaptation/">webpage</a> |
      <a href="https://arxiv.org/pdf/1904.01341.pdf">pdf</a> |
      <a href="javascript:toggleblock(&#39;assemblies19_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;assemblies19&#39;)" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/pdf/1904.01341.pdf">arXiv</a> |
      <a href="http://home.iitk.ac.in/~vinodkk/idda_model/idda_ppt">ppt</a> |
      <a href="https://github.com/vinodkkurmi/DiscriminatorDomainAdaptation">code</a>
      <br>

      <p align="justify"> <i id="assemblies19_abs" style="display: none;">In this paper, we tackle a problem of Domain Adaptation. In a domain adaptation setting, there is provided a labeled set of examples in a source dataset with multiple classes being present and a target dataset that has no supervision. In this setting, we propose an adversarial discriminator based approach. While the approach based on adversarial discriminator has been previously proposed; in this paper, we present an informed adversarial discriminator. Our observation relies on the analysis that shows that if the discriminator has access to all the information available including the class structure present in the source dataset, then it can guide the transformation of features of the target set of classes to a more structured adapted space. Using this formulation, we obtain the state-of-the-art results for the standard evaluation on benchmark datasets. We further provide detailed analysis which shows that using all the labeled information results in an improved domain adaptation.</i></p>

<pre xml:space="preserve" style="display: none;">@InProceedings{kurmi2019looking,
author = {Kurmi, Vinod Kumar and Namboodiri, Vinay P},
title = {Looking back at Labels: A Class based Domain Adaptation Technique},
booktitle = {International Joint Conference on Neural Networks (IJCNN) },
month = {July},
year = {2019}
}
</pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://delta-lab-iitk.github.io/CADA/"><img src="./Vinod_files/cvpr19.png" alt="sym" width="100%" style="border-radius:15px"></a>
    </td><td width="67%" valign="top">
      <p><a href="https://delta-lab-iitk.github.io/CADA/" id="CVPR19">
<!--       <img src="./Vinod_files/new.png"  width="6%" style="border-style: none"> -->
      <heading>Attending to Discriminative Certainty for Domain Adaptation</heading></a><br>
       <strong>Vinod K Kurmi*</strong>,  Shanu Kumar* , Vinay P. Namboodiri&nbsp;(*equal contribution)<br>
      <em>Conference on Computer Vision and Pattern Recognition (CVPR) </em>, 2019<br>
      </p>

      <div class="paper" id="iclr19">
      <a href="https://delta-lab-iitk.github.io/CADA/">webpage</a> |
      <a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Kurmi_Attending_to_Discriminative_Certainty_for_Domain_Adaptation_CVPR_2019_paper.html">pdf</a> |
      <a href="javascript:toggleblock(&#39;iclr19_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;iclr19&#39;)" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/1906.03502">arXiv</a> |
      <a href="http://home.iitk.ac.in/~vinodkk/cada/CADA_CVPR2019.pdf">poster</a>|
      <a href="https://github.com/DelTA-Lab-IITK/CADA">code</a>|
      <a href="http://home.iitk.ac.in/~vinodkk/cada/Supplementary_cada.pdf">supplementary</a>
      <!-- <a href="https://pathak22.github.io/large-scale-curiosity/index.html#sourceCode">code</a> -->

      <p align="justify"> <i id="iclr19_abs" style="display: none;">n this paper, we aim to solve for unsupervised domain adaptation of classifiers where we have access to label information for the source domain while these are not available for a target domain. While various methods have been proposed for solving these including adversarial discriminator based methods, most approaches have focused on the entire image based domain adaptation. In an image, there would be regions that can be adapted better, for instance, the foreground object may be similar in nature. To obtain such regions, we propose methods that consider the probabilistic certainty estimate of various regions and specific focus on these during classification for adaptation. We observe that just by incorporating the probabilistic certainty of the discriminator while training the classifier, we are able to obtain state of the art results on various datasets as compared against all the recent methods. We provide a thorough empirical analysis of the method by providing ablation analysis, statistical significance test, and visualization of the attention maps and t-SNE embeddings. These evaluations convincingly demonstrate the effectiveness of the proposed approach.</i></p>

<pre xml:space="preserve" style="display: none;">@InProceedings{Kurmi_2019_CVPR,
author = {Kumar Kurmi, Vinod and Kumar, Shanu and Namboodiri, Vinay P.},
title = {Attending to Discriminative Certainty for Domain Adaptation},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition(CVPR),},
month = {June},
year = {2019}
}
</pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://badripatro.github.io/MDN-VQG/"><img src="./Vinod_files/emnlp18.png" alt="sym" width="100%" style="border-style: none"></a>
    </td><td width="67%" valign="top">
      <p><a href="https://badripatro.github.io/MDN-VQG/" id="emnlp18">
      <heading>Multimodal Differential Network for Visual Question Generation</heading></a><br>
       Badri N Patro, Sandeep Kumar, <strong>Vinod K Kurmi</strong>, Vinay P Namboodiri<br>
      <em>Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 2018<br>
     <!--  <em><a href="https://sites.google.com/view/cvpr2018-robotic-vision/home" style="color:black" target="_blank">Deep Learning in Robotics Vision Workshop</a> (CVPR)</em>, 2018 (Oral)<br> -->
      </p>

      <div class="paper" id="cvprw18">
      <a href="https://badripatro.github.io/MDN-VQG/">webpage</a> |
      <a href="https://arxiv.org/pdf/1808.03986.pdf">pdf</a> |
      <a href="javascript:toggleblock(&#39;cvprw18_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;cvprw18&#39;)" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/pdf/1808.03986.pdf">arXiv</a> |
      <a href="https://github.com/vinodkkurmi/MDN-VQG/">code</a>
      <br>

      <p align="justify"> <i id="cvprw18_abs" style="display: none;">Generating natural questions from an image is a semantic task that requires using visual and language modality to learn multimodal representations. Images can have multiple visual and language contexts that are relevant for generating questions namely places, captions, and tags. In this paper, we propose the use of exemplars for obtaining the relevant context. We obtain this by using a Multimodal Differential Network to produce natural and engaging questions. The generated questions show a remarkable similarity to the natural questions as validated by a human study. Further, we observe that the proposed approach substantially improves over state-of-the-art benchmarks on the quantitative metrics (BLEU, METEOR, ROUGE, and CIDEr).</i></p>

<pre xml:space="preserve" style="display: none;">@inproceedings{patro2018multimodal,
  title={Multimodal Differential Network for Visual Question Generation},
  author={Patro, Badri Narayana and Kumar, Sandeep and Kurmi, Vinod Kumar and Namboodiri, Vinay},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={4002--4012},
  year={2018}
}
</pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/1807.07560"><img src="./Vinod_files/coling18.png" alt="sym" width="100%" style="border-radius:15px"></a>
    </td><td width="67%" valign="top">
      <p><a href="https://arxiv.org/abs/1807.07560" id="CompGAN18">
      <heading>Learning Semantic Sentence Embeddings using Sequential Pair-wise Discriminator</heading></a><br>
      Badri N Patro*,<strong>Vinod K Kurmi*</strong>, Sandeep K*, Vinay P Namboodiri&nbsp;(*equal contribution)<br>
      <em>International Conference on Computational Linguistics (COLING)</em>, 2018<br>
      </p>

      <div class="paper" id="compgan18">
     <a href="https://github.com/vinodkkurmi/PQG/">webpage</a> |
      <a href="https://www.aclweb.org/anthology/C18-1230">pdf</a> |
      <a href="javascript:toggleblock(&#39;cvprw18_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;cvprw18&#39;)" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/1806.00807">arXiv</a> |
      <a href="https://github.com/vinodkkurmi/PQG/">code</a>
      <!-- <a href="https://pathak22.github.io/large-scale-curiosity/index.html#sourceCode">code</a> -->
      <br>

      <p align="justify"> <i id="compgan18_abs" style="display: none;">In this paper, we propose a method for obtaining sentence-level embeddings. While the problem of securing word-level embeddings is very well studied, we propose a novel method for obtaining sentence-level embeddings. This is obtained by a simple method in the context of solving the paraphrase generation task. If we use a sequential encoder-decoder model for generating paraphrase, we would like the generated paraphrase to be semantically close to the original sentence. One way to ensure this is by adding constraints for true paraphrase embeddings to be close and unrelated paraphrase candidate sentence embeddings to be far. This is ensured by using a sequential pair-wise discriminator that shares weights with the encoder that is trained with a suitable loss function. Our loss function penalizes paraphrase sentence embedding distances from being too large. This loss is used in combination with a sequential encoder-decoder network. We also validated our method by evaluating the obtained embeddings for a sentiment analysis task. The proposed method results in semantic embeddings and outperforms the state-of-the-art on the paraphrase generation and sentiment analysis task on standard datasets. These results are also shown to be statistically significant.</i></p>

<pre xml:space="preserve" style="display: none;">@inproceedings{patro2018learning,
  title={Learning Semantic Sentence Embeddings using Sequential Pair-wise Discriminator},
  author={Patro, Badri Narayana and Kurmi, Vinod Kumar and Kumar, Sandeep and Namboodiri, Vinay},
  booktitle={Proceedings of the 27th International Conference on Computational Linguistics},
  pages={2715--2729},
  year={2018}
}
</pre>
      </div>
    </td>
  </tr>

  <tr>
    <!-- <td width="33%" valign="top" align="center"><a href="http://home.iitk.ac.in/~vinodkk/pdf_files/gesture_paper"><img src="./Vinod_files/depth.png" alt="sym" width="65%" border="1" style="border-color:black"><hr style="height:0pt; visibility:hidden; margin:0"><img src="./Deepak Pathak_files/iclr18_2.gif" alt="sym" width="65%" border="1" style="border-color:black"></a> -->
 <!--    </td> -->
    <td width="33%" valign="top" align="center"><a href="http://home.iitk.ac.in/~vinodkk/pdf_files/gesture_paper"><img src="./Vinod_files/gesture2.png" alt="sym" width="100%" style="border-radius:15px"></a>
    </td>
    <td width="67%" valign="top">
      <p><a href="http://home.iitk.ac.in/~vinodkk/pdf_files/gesture_paper" id="wscg">
      <heading>Robust hand gesture recognition from 3D data</heading></a><br>
      <strong>Vinod K Kurmi</strong>,Garima Jain, Venkatesh K Subramanian <br>
      <em>International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision(WSCG)</em>, 2015<br>
      <!-- <strong style="color:red">Oral presentation <a href="https://vimeo.com/237270588" target="_blank" style="color:red">[video]</a></strong> -->
      <!-- <strong style="color:black">(Oral presentation)</strong> -->
      </p>

      <div class="paper" id="iclr18">
      <a href="http://home.iitk.ac.in/~vinodkk/pdf_files/gesture_paper">pdf</a> |
      <!-- <a href="http://pathak22.github.io/zeroshot-imitation/resources/iclr18.pdf">pdf</a> | -->
      <a href="javascript:toggleblock(&#39;iclr18_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;iclr18&#39;)" class="togglebib">bibtex</a> |
      <a href="http://wscg.zcu.cz/WSCG2015/!_2015_WSCG_SHORT_proceedings.pdf" target="_blank">weblink</a> |
      <a href="https://github.com/vinodkkurmi/">code</a> |
      <a href="http://www.iitk.ac.in/siic/d/content/human-hand-detection-system-apparatus-and-method-thereof">webpage</a> |
      <a href="https://www.youtube.com/watch?v=ejyaQ4N2waI&feature=youtu.be">video</a>

      <br>

      <p align="justify"> <i id="iclr18_abs" style="display: none;">In this paper, we use the output of a 3D sensor (ex. Kinect from Microsoft) to capture depth images of humans making a set of predefined hand gestures in various body poses. Conventional approaches using Kinect data have been constrained by the limitation of the human detector middleware that requires close conformity to a standard near erect, legs apart, hands apart pose for the subject. Our approach also permits clutter and possible motion in the scene background, and to a limited extent, in the foreground as well. We make an important point in this work to emphasize that the recognition performance is considerably improved by a choice of hand gestures that accommodate the sensor’s specific limitations. These sensor limitations include low resolution in x and y as well as z. Hand gestures have been chosen (designed) for easy detection by seeking to detect a fingers apart, fingertip constellation with minimum computation. without, however compromising on issues of utility or ergonomy. It is shown that these gestures can be recognised in real time irrespective of visible band illumination levels, background motion, foreground clutter, user body pose, gesturing speeds and user distance. The last is of course limited by the sensor’s own range limitations. Our main contributions are the selection and design of gestures suitable for limited range, limited resolution 3D sensors and the novel method of depth slicing used to extract hand features from the background. This obviates the need for preliminary human detection and enables easy detection and highly reliable and fast (30 fps) gesture classification.</i></p>

<pre xml:space="preserve" style="display: none;">@article{kurmi2015robust,
  title={Robust hand gesture recognition from 3D data},
  author={Kurmi, Vinod K and Jain, Garima and Venkatesh, KS},
  year={2015},
  publisher={V{\'a}clav Skala-UNION Agency}
}
</pre>

    </div>
    </td>
  </tr>

  <tr>
    <!-- <td width="33%" valign="top" align="center"><a href="http://home.iitk.ac.in/~vinodkk/pdf_files/gesture_paper"><img src="./Vinod_files/depth.png" alt="sym" width="65%" border="1" style="border-color:black"><hr style="height:0pt; visibility:hidden; margin:0"><img src="./Deepak Pathak_files/iclr18_2.gif" alt="sym" width="65%" border="1" style="border-color:black"></a> -->
 <!--    </td> -->
    <td width="33%" valign="top" align="center"><a href="http://home.iitk.ac.in/~vinodkk/pdf_files/gesture_paper"><img src="./Vinod_files/depth.png" alt="sym" width="100%" style="border-radius:15px"></a>
    </td>
    <td width="67%" valign="top">
      <p><a href="http://home.iitk.ac.in/~vinodkk/pdf_files/gesture_paper" id="wscg">
      <heading>A Human-Hand Detection System, Apparatus and A Method Thereof</a><br>
      <strong>Vinod K Kurmi</strong>,Garima Jain, Venkatesh K Subramanian <br>
      <em>Indian Patent</em>, 2016<br>
      <!-- <strong style="color:red">Oral presentation <a href="https://vimeo.com/237270588" target="_blank" style="color:red">[video]</a></strong> -->
      <!-- <strong style="color:black">(Oral presentation)</strong> -->
      </p>

      <div class="paper" id="iclr18">
      <!-- <a href="http://pathak22.github.io/zeroshot-imitation/resources/iclr18.pdf">pdf</a> | -->
      <a href="javascript:toggleblock(&#39;iclr18_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;iclr18&#39;)" class="togglebib">bibtex</a> |
      <a href="http://www.ipindia.nic.in/writereaddata/Portal/IPOJournal/1_419_1/Part1.pdf" target="_blank">weblink</a> |
      <a href="http://www.iitk.ac.in/siic/d/content/human-hand-detection-system-apparatus-and-method-thereof">webpage</a> |
      <a href="https://www.youtube.com/watch?v=ejyaQ4N2waI&feature=youtu.be">video</a>

      <br>

      <p align="justify"> <i id="iclr18_abs" style="display: none;">In this paper, we use the output of a 3D sensor (ex. Kinect from Microsoft) to capture depth images of humans making a set of predefined hand gestures in various body poses. Conventional approaches using Kinect data have been constrained by the limitation of the human detector middleware that requires close conformity to a standard near erect, legs apart, hands apart pose for the subject. Our approach also permits clutter and possible motion in the scene background, and to a limited extent, in the foreground as well. We make an important point in this work to emphasize that the recognition performance is considerably improved by a choice of hand gestures that accommodate the sensor’s specific limitations. These sensor limitations include low resolution in x and y as well as z. Hand gestures have been chosen (designed) for easy detection by seeking to detect a fingers apart, fingertip constellation with minimum computation. without, however compromising on issues of utility or ergonomy. It is shown that these gestures can be recognised in real time irrespective of visible band illumination levels, background motion, foreground clutter, user body pose, gesturing speeds and user distance. The last is of course limited by the sensor’s own range limitations. Our main contributions are the selection and design of gestures suitable for limited range, limited resolution 3D sensors and the novel method of depth slicing used to extract hand features from the background. This obviates the need for preliminary human detection and enables easy detection and highly reliable and fast (30 fps) gesture classification.</i></p>

<pre xml:space="preserve" style="display: none;">@article{kurmi2015robust,
  title={Robust hand gesture recognition from 3D data},
  author={Kurmi, Vinod K and Jain, Garima and Venkatesh, KS},
  year={2015},
  publisher={V{\'a}clav Skala-UNION Agency}
}
</pre>


<table width="100%" align="center" border="0" cellpadding="10">
  <tbody><tr><td>
    <sectionheading>&nbsp;&nbsp;Selected Awards</sectionheading>
    <ul>
    <li>  <strong>  <a href="https://www.daad.de/en/the-daad/postdocnet/fellows/fellows/#Kurmi
"> AI Post-Doc Net Fellow</a> </strong> (2020-2021)</li>
    <li>  <strong>TCS Research Fellowship </strong> (2016-2019)</li>
    <li>  <strong>Qualcomm Innovation Fellowship </strong> 2019 Finalists (2019)</li>
    <li> MHRD PhD Fellowship (2015-16)</li>
    <li> Qualified for  <strong>Junior Research Fellowship(JRF)</strong> of CSIR-NET(2014)</li>
    <li>  MHRD Post-Graduate Fellowship (2012-14)</li>
    </ul>
  </td></tr>
</tbody></table>

<!--      </tbody></table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td>
        <heading>Affiliations</heading>
        </td>
      </tr>
      </tbody></table>
      <table align="center">
    <tbody><tr>
        <td width="17%" align="center">
            <a href="http://www.iitk.ac.in/" target="_blank">
            <img style="width:80px" src="./Unnat Jain_files/iitk.png"></a>&nbsp; &nbsp;
        </td>
        <td width="17%" align="center">
            <a href="https://www.umass.edu/" target="_blank">
            <img style="width:80px" src="./Unnat Jain_files/umass.png"></a>&nbsp; &nbsp;
        </td>
        <td width="17%" align="center">
            <a href="https://cs.illinois.edu/" target="_blank">
            <img style="width:100px" src="./Unnat Jain_files/uiuc2.jpg"></a>&nbsp; &nbsp;
        </td>
        <td width="17%" align="center">
            <a href="https://www.uber.com/info/atg/" target="_blank">
            <img style="width:120px" src="./Unnat Jain_files/uber2.png"></a>&nbsp; &nbsp;
        </td>
        <td width="17%" align="center">
            <a href="https://prior.allenai.org/" target="_blank">
            <img style="width:60px" src="./Unnat Jain_files/ai2_2.png"></a>&nbsp; &nbsp;
        </td>
        <td width="22%" align="center">
            <a href="https://research.fb.com/category/facebook-ai-research/" target="_blank">
            <img style="width:150px" src="./Unnat Jain_files/fair.png"></a>&nbsp; &nbsp;
        </td>
    </tr>
    <tr>
        <td width="17%" align="center"><font size="2">IIT Kanpur<br>2008-2016</font></td>
        <td width="17%" align="center"><font size="2">UMass Amherst<br>Summer 2015</font></td>
        <td width="17%" align="center"><font size="2">UIUC <br>2016-present</font></td>
        <td width="17%" align="center"><font size="2">Uber ATG<br>Summer 2017</font></td>
        <td width="17%" align="center"><font size="2">Allen Institute for AI<br>Summer 2018, 2020</font></td>
        <td width="22%" align="center"><font size="2">FAIR<br>Summer 2019</font></td>

    </tr>
</tbody></table> -->










<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr><td><br><p align="right"><font size="2">
    Template: <a href="http://www.cs.berkeley.edu/~barron/">this</a>, <a href="http://www.cs.berkeley.edu/~sgupta/">this</a> and <a href="http://jeffdonahue.com/">this</a>
    </font></p></td></tr>
</tbody></table>

  </td></tr>
</tbody></table>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('jpm15_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('fg15_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('wacv15_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('iclr15_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('cvpr15_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('iccv15_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('jmlr16_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('cvpr16_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('iclr16_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('cvpr17_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('icml17_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('nips17_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('iclr18_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('icml18_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('cvprw18_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('iclr19_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('assemblies19_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('compgan18_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('bmvc19_abs');
</script>



</body></html>
