<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0032)https://www.cs.cmu.edu/~dpathak/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Credits: Jon Barron and Abhishek Kar and Saurabh Gupta*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 17px; /* 19 */
    font-weight: 600 /* 1000 */
  }
  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 600 /* 800 */
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }
  .ImageBorder
  {
      border-width: 1px;
      border-color: Black;
  }
  </style>
<!--   <link rel="shortcut icon" href="https://www.cs.cmu.edu/sites/default/files/favicon_0.ico" type="image/vnd.microsoft.icon">
 -->
 <script type="text/javascript" src="./vinod_files/hidebib.js"></script>
  <title>Vinod K Kurmi</title>
  <meta name="Vinod K Kurmi&#39;s Homepage" http-equiv="Content-Type" content="Vinod K Kurmi&#39;s Homepage">
  <link href="./vinod_files/css" rel="stylesheet" type="text/css">

  <!-- Scramble Script by Jeff Donahue -->
  <script src="./vinod_files/scramble.js"></script>
</head>

<body data-new-gr-c-s-check-loaded="14.990.0" data-gr-ext-installed="">
<table width="840" border="0" align="center" cellspacing="0" cellpadding="20">
  <tbody><tr><td>

<p align="center">

    <!-- <font size="7">Vinod K Kurmi</font><br> -->
    <pageheading>Vinod K Kurmi</pageheading><br><!-- 
     <b>Email</b>: vinodkk@iiserb.ac.in, vinodkumarkurmi@gmail.com
    -->  <b>Email</b>:&nbsp
    <font id="email" style="display:inline;">
      <noscript><i>Please enable Javascript to view</i></noscript>
    </font>
    <script>
    emailScramble = new scrambledString(document.getElementById('email'),
        'emailScramble', 'kdo.in@iinrvcksibea.',
        [6,5,4,15,2,3,8,19,10,20,13,1,17,7,11,9,14,12,16,18]);
    </script>
 <br>
    <font id="email2" style="display:inline;">
    </font>
   
    <script>
    emailScramble2 = new scrambledString(document.getElementById('email2'),
        'emailScramble2', '@muinidkmuivgokaacmlm.ror',
        [16,18,7,20,3,2,5,6,25,12,15,1,17,24,11,9,19,23,8,21,14,22,10,4,13]);
    </script>
</br>
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tbody><tr>
    <td width="35%" valign="top"><a href="https://vinodkkurmi.github.io/"><img src="./vinod_files/pr5.jpg" width="110%" style="border-radius:15px"></a>
    <p align="center">
    | <a href="./vinod_files/vinodkk_cv.pdf">CV</a> | <a href="https://scholar.google.co.in/citations?user=Exo2VNAAAAAJ&hl=en">Google Scholar</a> |<br>|
    <a href="https://github.com/vinodkkurmi">Github</a> |
    <a href="https://twitter.com/vinodkkurmi">Twitter</a> |
    </p>
    <p align=center style="margin-top:-8px;" ><a href="https://twitter.com/vinodkkurmi?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @vinodkkurmi</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
    </p>
    
       <p>
   <b>Office Address:</b> 311B, AB4 (Therm Building), IISER Bhopal. </p>
    <p> <b> Phone No.:</b> +91 755 269 2689 </p>
     <b>Email</b>:&nbsp
    <font id="email3" style="display:inline;">
      <noscript><i>Please enable Javascript to view</i></noscript>
    </font>
    <script>
    emailScramble3 = new scrambledString(document.getElementById('email3'),
        'emailScramble3', 'kdo.in@iinrvcksibea.,',
        [6,5,4,15,2,3,8,19,10,20,13,1,17,7,11,9,14,12,16,18,21]);
    </script>
 <br>
    <font id="email4" style="display:inline;">
    </font>
   
    <script>
    emailScramble4 = new scrambledString(document.getElementById('email4'),
        'emailScramble4', '@muinidkmuivgokaacmlm.ror',
        [16,18,7,20,3,2,5,6,25,12,15,1,17,24,11,9,19,23,8,21,14,22,10,4,13]);
    </script>
</br>

</p>
    </td>
    <td width="68%" valign="top" align="justify">
    <p> I am an assistant professor at <a href="https://dse.iiserb.ac.in/">Dept. of DSE</a>, <a href="https://www.iiserb.ac.in/">IISER Bhopal</a>. 
<p> <b>Brief Bio:</b>
    Before joining IISER Bhopal, I was a post-doc research fellow at <a href="https://www.kuleuven.be/english//">KU Leuven, Belgium</a>, worked with <a href="https://scholar.google.co.in/citations?user=EuFF9kUAAAAJ&hl=en">Prof. Tinne Tuytelaars</a> and  <a href="https://cvit.iiit.ac.in/">CVIT Hyderabad</a>, worked with <a href="https://scholar.google.co.in/citations?user=U9dH-DoAAAAJ&hl=en">Prof. C.V. Jawahar</a> respectively. I was a <a href="https://www.daad.de/en/the-daad/postdocnet/fellows/fellows/#Kurmi">DAAD AI Post-Doc-Net Fellow. </a>
        </p>
        I obtained my Ph.D. from  <a href="http://www.iitk.ac.in/">Indian Institute of Technology Kanpur, India</a> under the supervision of <a href="http://www.cse.iitk.ac.in/users/vinaypn/">Prof. Vinay P Namboodiri</a> and <a href="http://home.iitk.ac.in/~venkats/">Prof. K. S. Venkatesh</a>. I was associated with the <a href="http://deltalab.iitk.ac.in/">Delta Lab</a> and <a href="http://iitk.ac.in/ee/computer-vision-lab">Computer Vision Lab</a> and was a TCS-Research fellow from 2015-2020. I completed my M.Tech in 2014 from  <a href="http://www.iitk.ac.in/"> Indian Institute of Technology Kanpur</a> under the supervision of <a href="http://home.iitk.ac.in/~venkats/">Prof. K. S. Venkatesh</a>. I  graduated in 2012 from <a href="http://www.sgsits.ac.in/">SGSITS Indore</a>. 
        </p>

        <p>
   <b>Research:</b> My research areas are related to computer vision (CV), deep learning (DL), and machine learning (ML). 
    Some of the active areas of research include:
      <ul>
        <li> Transfer learning (Domain adaptation, Incremental learning).</li>
        <li> Fairness and bias-free learning.</li>
        <li> Uncertainty in deep learning (Bayesian models).</li>
         <li>Data generative models (GANs, VAEs, and Diffusion models).</li>
        <li> Medical image processing.</li>
         <li>3D reconstructions, augmented reality (AR), and virtual reality (VR).</li>
          <li>Speech and audio analysis.</li>
      </ul>
      For more details please visit the <a href="https://vinodkkurmi.github.io/dcl">Visual Data Computing Group (VisDom)</a>.
    </p>


    </td>
  </tr>
</tbody></table>

<hr>
<!-- <div class="">
                <p class="sz15 bold"><marquee behavior="scroll" direction="left" scrollamount="10">Code is Coming Soon! Stay Tuned!!</marquee></p>
            </div> -->

  
We are seeking motivated researchers who are passionate about computer vision, machine learning, and deep learning. If you are interested, please send us your CV and highlight your areas of interest and experience to <a style="color:red;">visdomdse@gmail.com</a>.
<!-- <blink> Current Opening for JRF/Project associates:  </blink> -->
<!--  <marquee direction="right"
             behavior="alternate"
             style="border:BLACK 0px SOLID">
        <b>Current Opening for JRF/Project associates in Deep learning based project: </b>
<a href="https://www.iiserb.ac.in/assets_external/vacancy/r&d/ebe7d0f60a4db206474e36139127e034.pdf"> [Link]</a> 

    </marquee> -->


<!--  <marquee direction="left"
             behavior="alternate"
             style="border:BLACK 0px SOLID">
    
 <b>Current Opening for Project associates-I in Medical Imaging based project:  </b>
<a href="https://www.iiserb.ac.in/assets_external/vacancy/r&d/f5ccfe3baac5a0309c01306ce2004ed4.pdf"> [Link]</a> 
    </marquee> -->


<!-- <marquee> <b>Current Opening for JRF/Project associates: </b>
<a href="https://www.iiserb.ac.in/assets_external/vacancy/r&d/7e41e874f3a015fac40bd72f6a0ddbd9.pdf"> [Link]</a> </marquee> -->

<!-- <marquee behavior="scroll" direction="right" scrollamount="12">Little Fast Scrolling</marquee> -->



<p>
For more details please visit the Visual Data Computing Group (<a href="https://vinodkkurmi.github.io/dcl">VisDom</a>).</p>

<!-- <p>
Useful links: <a href="https://www.iiserb.ac.in/doaa/internship/">[IISER Bhopal Internship Program], </a>
<a href="https://www.iiserb.ac.in/doaa/admission"> [PhD Program]</a>, 
<a href="https://www.iiserb.ac.in/assets/off_academic_affairs/forms_and_formats/Application%20Form%20for%20Dissertation.pdf"> [Dissertation form Non-IISERB]</a> -->


 <!-- <img src="./vinod_files/new.png"  width="4%" style="border-style: none"> Junior research fellow/project associates vacancy: <a href="https://www.iiserb.ac.in/assets_external/vacancy/r&d/67691f6762aa10f0df5d7c4d3c22fbb3.pdf">[Details]</a> -->
<!-- <a> href="https://www.iiserb.ac.in/doaa/internship" >IISER Bhopal Internship Program</a>
 <>a href="https://ww1.iiserb.ac.in/doaa/phd-admission"> PhD programme</a -->

<hr>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">

  <tbody><tr><td>
    <sectionheading>&nbsp;&nbsp;News</sectionheading>
    
     <!-- <heading>&nbsp;&nbsp;News</heading> -->
    
     <li> Paper accepted in <a href="https://www.interspeech2025.org/home" >Interspeech-2025 </a></li>
      <li> Invited for a discussion on <a href=" ./vinod_files/air.jpeg" >"AI and our Future"</a>  at <a href=" https://www.newsonair.gov.in/regional-units/bhopal/" >Akashvani, Bhopal .</a></li>
         <li> 3 Papers accepted in <a href="https://wacv2025.thecvf.com/" >WACV-25.</a></li>
          <li> Invited talk on 'AI on Radiantion Oncology' at ICRO Proadvance 2024 in PGIMER Chandigarh. </a></li>

        <li> 'HetGP- Quantifying Uncertainty  through Residuals' has been accepted at <a href="https://cikm2024.org/" >CIKM-24.</a></li>
      <li> 'CosFairNet' has been accepted at <a href="https://bmvc2024.org/" >BMVC-24.</a></li>
     <li> Our work 'Few-shot Audio Class Incremental Learning' has been accepted at <a href="https://interspeech2024.org/" >Interspeech-24.</a></li>
      <li> Patent granted  <a href="https://iprsearch.ipindia.gov.in/RQStatus/PatentCertificatePDF.aspx?AppNo=MTY3OC9ERUwvMjAxNQ==&FullPath=LVBhdGVudENlcnRpZmljYXRlMTQtMDMtMjAyNC5wZGY=">[Certificate]</a>.</a></li>    
  
       

    <a href="javascript:toggleblock(&#39;news&#39;)">---- show more ----</a>
    <div id="news" style="display:none">
      <li>Selected to attend the  <a href="  https://event.india.acm.org/pic/home/" >ACM-Pingala Interactions 2024</a>.</a></li>    
      <li>  Our work on '3D Face Reconstruction' has been accepted at  <a href="https://www.journals.elsevier.com/image-and-vision-computing" >Image Vision Computing</a> Journal-Elsevier.</li>
         <li> Received a travel grant from the DAAD AI-Net fellowship to visit several German Research and academic institutes. </a></li>  
                <li> Invited for an expert talk on "Basics of AI" at Science Day in Govt Polytechnic College, Sironj, Vidisha. </a></li>
        <li> Invited talk on "AI: Past, Present and Future" at ScienceFiesta2023 in Regional Science Centre, Bhopal </a></li>
        <li> Our work on 'Audio Keyword Spotting' has been accepted at <a href="https://interspeech2022.org/" >InterSpeech-22.</a></li>
                <li> Invited talk on "Diaster Management using CV" at Digital University Kerala.</a></li>
        <li> Joined as an assistant professor in DSE, IISER Bhopal</a></li>
        <li> Received  "Outstanding PhD Thesis Award" from IIT Kanpur.</a></li>
      <li> Our work on 'Bias-free learning' has been accepted at <a href="https://aaai.org/Conferences/AAAI-22/" >AAAI-22.</a></li>
      <li> Our work on 'Context learning-based 3D face generation' has been accepted at <a href="https://wacv2022.thecvf.com/" >WACV-22.</a></li>
      <li> <a href="https://www.journals.elsevier.com/image-and-vision-computing" >ICCV 2021 </a>Doctoral Consortium.</li>
         <li> 'Uncertainty based VQG (MUMC)' has been accepted at <a href="https://iccv2021.thecvf.com/home" >Image Vision Computing</a> Journal - Elsevier.</li>
        <li> 'Dropout Domain Adaptation' has been accepted at <a href="https://www.journals.elsevier.com/neurocomputing" >NeuroComputing</a> Journal - Elsevier .</li>
      <li> Join as a Post-Doc Research Fellow at <a href="https://www.kuleuven.be/english/">KU Leuven </a> Belgium. </li>
      <li> 'Informative Discriminator in DA' has been accepted at  <a href="https://www.journals.elsevier.com/image-and-vision-computing" >Image Vision Computing</a> Journal - Elsevier.</li>
      <li> Two papers on 'Fingerprint enchnacement and domain adaptation' have been accepted at <a href="https://www.ijcnn.org/" >IJCNN-21</a> .</li>
       <li> Our 'Multimodal Audio-Video generation' paper got accepted at <a href="https://2021.ieeeicassp.org" >ICASSP-21</a> .</li>
    <li> Defended PhD thesis on  "Understading Transfer Learning between Domains and Tasks".</li>
    <li> Paper accepted at workshop of <a href="http://vcmi.inesctec.pt/xai4biom_wacv/index.html
">xAI4Biometrics</a> at WACV-21.</li>
<li> Two papers are accepted at <a href="http://wacv2021.thecvf.com/"> WACV-21</a>.</li>
  <li> Selected as a DAAD <a href="https://www.daad.de/en/the-daad/postdocnet/fellows/fellows/#Kurmi"> AI Post-Doc Net Fellow </a>. </li>
     <li> Join as a Post-Doc at <a href="https://cvit.iiit.ac.in/">CVIT </a> Hyderabad. </li>
    <li> Paper accepted in <a href="https://www.journals.elsevier.com/neurocomputing"> Neurocomputing </a>. </li>
    <li> Delivered Open Seminar on "Understading Transfer Learning between Domains and Tasks"</li>
    <li>Two papers accepted in  Workshop on Closing the Loop Between Vision and Language<a href="https://sites.google.com/site/iccv19clvllsmdc/program"> CLVL</a> at ICCV-19</li>
        <li>Paper accepted in <a href="http://wacv2020.thecvf.com/"> WACV-20</a></li>

    <li>Particiapted in  Amazon Research Day-2019 at Bangalore</li>

    <li>Paper accepted in <a href="https://bmvc2019.org/"> BMVC-19</a></li>

    <li>Presented our work at  <a href="https://ieeexplore.ieee.org/document/8492375"> IJCNN</a> Hungary.</li>

    <li>Presented our work at  <a href="https://cvpr2019.thecvf.com/"> CVPR</a> Long Beach, US.</li>
    <li> Delivered talk on "Baysian Domain Adaptation" at <a href="https://www.qualcomm.com/invention/research/university-relations/innovation-fellowship/finalists/"> Qualcomm Bangalore</a> </li>
        <li>Named <a href="https://www.qualcomm.com/invention/research/university-relations/innovation-fellowship/finalists">Qualcomm Innovation Fellowship 2019</a> Finalists.</li>

    <li> Paper accepted in <a href="https://ieeexplore.ieee.org/document/8492375"> IJCNN-19</a></li>

    <li> Paper accepted in <a href="https://cvpr2019.thecvf.com/"> CVPR-19</a></li>

    <li> Particiapted in  Amazon Research Day-2018 at Bangalore</li>


    </div>
    <!-- <li> Undergrad <a href="#JPM15">paper</a> related to predicting <a href="http://www.huffingtonpost.co.uk/2015/02/23/microsoft-oscar-predictio_n_6735122.html">Oscars</a> published at <a href="http://ubplj.org/index.php/jpm/article/view/1048">JPM</a>. See <a href="#oscarPred">live predictions</a>.</li> -->
    </ul>
  </td></tr>
</tbody></table>

<hr>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tbody><tr><td><sectionheading>&nbsp;&nbsp;Publications</sectionheading></td></tr>
       <!-- <hr> -->
</tbody></table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">

    <tbody>


<!-- StarttPaper one -->
<tr>
  <td width="33%" valign="top" align="center" style="padding-top:0px"><a href="https://visdomlab.github.io/"><img src="./vinod_files/interspeech25.png" alt="sym" width="100%" style="padding:0px;border-radius:15px;border:0px solid black"></a></td>
      <td width="67%" valign="top" style="padding-top:10px">
    <p><a href="https://visdomlab.github.io/" id="interspeech25_id">
       <img src="./vinod_files/new.png"  width="6%" style="border-style: none">
<heading>Multilingual Query-by-Example KWS using Transliteration</heading></a><br>
Kirandevraj R, <strong>Vinod K Kurmi</strong>, Vinay P Namboodiri, C V Jawahar<br>
<em>Conference of the International Speech Communication Association (Interspeech) </em> 2025, Rotterdam, The Netherlands.
</p>
<div class="paper" id="interspeech25">
<a href="javascript:toggleblock(&#39;interspeech25_abs&#39;)">abstract</a> |
<!-- <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2022/r22_interspeech.pdf">pdf</a> | -->
<a shape="rect" href="javascript:togglebib(&#39;interspeech25&#39;)" class="togglebib">bibtex</a> |
  

  <p align="justify"> <i id="interspeech25_abs" style="display: none;"> Query-by-Example Keyword Spotting (QbE KWS) detects query audio within target audio. A common approach for multilingual QbE KWS uses phoneme posteriors as representations, with a shared phoneme dictionary across languages. We propose a novel method that replaces phoneme-based representations with transliteration, unifying transcripts from multiple Indian languages into the Devanagari script, a text script used for Hindi and Marathi. We train a Multilingual ASR model to predict transliterated Devanagari text from audio across 10 Indian languages. The character logits from this ASR serve as both query and target audio features. Using the Kathbath dataset for training and the IndicSUPERB QbE evaluation set, our approach achieves significant improvements. The average MTWV increased from 0.015 (IndicSUPERB) to 0.504, and performance rose from 0.387 to 0.504, surpassing the best-performing Marathi ASR baseline. This demonstrates the effectiveness of transliteration for multilingual KWS.</i></p>

  <pre xml:space="preserve" style="display: none;">@inproceedings{kiran_inter25,
    Author = {R,Kiran.
    and Kurmi, Vinod K
    and Namboodiri, Vinay P
    and Jawhar, CV},
    Title = {Multilingual Query
    -by-Example KWS using 
    Transliteration},
    Booktitle = {InterSpeech},
    Year   = {2025}
    }
</pre>
    </div>
  </td>
</tr>












<!-- StarttPaper one -->
   <tr>
    <td width="33%" valign="top" align="center" style="padding-top:0px"><a href="https://visdomlab.github.io/FeatAugFSCIL/"><img src="./vinod_files/wacv24_inc.png" alt="sym" width="100%" style="padding:0px;border-radius:15px;border:0px solid black"></a></td>
        <td width="67%" valign="top" style="padding-top:10px">
      <p><a href="https://visdomlab.github.io/FeatAugFSCIL/" id="wacv25_inc_id">
         <!-- <img src="./vinod_files/new.png"  width="6%" style="border-style: none"> -->
  <heading>Strategic Base Representation Learning via Feature Augmentations for Few-Shot Class Incremental Learning</heading></a><br>
       Parinita Nema,  <strong>Vinod K Kurmi</strong><br>
             <em>IEEE Winter Conference of Applications on Computer Vision (WACV)</em>, 2025.

      </p>
      <div class="paper" id="wacv25_inc">
      <a href="javascript:toggleblock(&#39;wacv25inc_abs&#39;)">abstract</a> |
        <a href="https://visdomlab.github.io/FeatAugFSCIL/">webpage</a> |
      <a href="https://arxiv.org/abs/2501.09361">ArXiv</a> |
      <a href="https://github.com/Parinitanema/FACL">code</a> |
      <a shape="rect" href="javascript:togglebib(&#39;wacv25_inc&#39;)" class="togglebib">bibtex</a> |
     

      <p align="justify"> <i id="wacv25inc_abs" style="display: none;"> Few-shot class incremental learning implies the model to learn new classes while retaining knowledge of previously learned classes with a small number of training instances. Existing frameworks typically freeze the parameters of the previously learned classes during the incorporation of new classes. However, this approach often results in suboptimal class separation of previously learned classes,  leading to overlap between old and new classes. Consequently, the performance of old classes degrades on new classes. To address these challenges, we propose a novel feature augmentation-driven contrastive learning framework designed to enhance the separation of previously learned classes to accommodate new classes. Our approach involves augmenting feature vectors and assigning proxy labels to these vectors. This strategy expands the feature space, ensuring seamless integration of new classes within the expanded space. Additionally, we employ a self-supervised contrastive loss to improve the separation between previous classes. We validate our framework through experiments on three FSCIL benchmark datasets: CIFAR100, miniImageNet, and CUB200. The results demonstrate that our Feature Augmentation driven Contrastive Learning framework significantly outperforms other approaches, achieving state-of-the-art performance.</i></p>

 <pre xml:space="preserve" style="display: none;">@inproceedings{nema_wacv25,
Author = {Nema, Parinita 
and Kurmi, Vinod },
Title = {Strategic Base Representation
Learning via Feature Augmentations for
Few-Shot Class Incremental Learning},
Booktitle = {WACV},
Year   = {2025}
}
</pre>
      </div>
    </td>
  </tr>



<!-- StarttPaper one -->
   <tr>
    <td width="33%" valign="top" align="center" style="padding-top:0px"><a href="https://visdomlab.github.io/DUEB/"><img src="./vinod_files/wacv25_ss.png" alt="sym" width="100%" style="padding:0px;border-radius:15px;border:0px solid black"></a></td>
        <td width="67%" valign="top" style="padding-top:10px">
      <p><a href="https://visdomlab.github.io/DUEB/" id="wacv25_ss_id">
         <!-- <img src="./vinod_files/new.png"  width="6%" style="border-style: none"> -->
  <heading>Uncertainty and Energy based Loss Guided Semi-Supervised Semantic Segmentation</heading></a><br>
        Rini Thakur,  <strong>Vinod K Kurmi</strong><br>
             <em>IEEE Winter Conference of Applications on Computer Vision (WACV)</em>, 2025.

      </p>
      <div class="paper" id="wacv25_ss">
      <a href="javascript:toggleblock(&#39;wacv25ss_abs&#39;)">abstract</a> |
        <a href="https://visdomlab.github.io/DUEB/">webpage</a> |
      <a href="https://www.arxiv.org/abs/2501.01640">ArXiv</a> |
      <!-- <a href="https://vinodkkurmi.github.io/">code</a> | -->
      <a shape="rect" href="javascript:togglebib(&#39;wacv25_ss&#39;)" class="togglebib">bibtex</a> |
     

      <p align="justify"> <i id="wacv25ss_abs" style="display: none;"> Semi-supervised semantic segmentation exploits both labeled and unlabeled images to overcome tedious and costly pixel-level annotation problem. Pseudolabel supervision is one of the core approaches of training networks with both pseudo labels and ground-truth labels. In this work, we use aleatoric or data uncertainty and energy based modeling in intersection-union pseudo supervised network. The inherent noise variations of the data are being modeled by the aleatoric uncertainty in a network with two predictive branches. The per-pixel variance parameter obtained from the network gives a quantitative idea about the data uncertainty. Moreover, energy-based loss realizes the potential of generative modeling on the downstream semi-supervised segmentation task. The aleatoric and energy loss is applied in conjunction with pseudo-intersection labels, pseudo-union labels, and ground-truth on the respective network branch. The comparative analysis with state-of-the-art methods has shown improvement in performance metrics.</i></p>

 <pre xml:space="preserve" style="display: none;">@inproceedings{thakur_wacv25,
Author = {Thakur, Rini 
and Kurmi, Vinod },
Title = {Uncertainty and Energy 
based Loss Guided Semi-Supervised
Semantic Segmentation},
Booktitle = {WACV},
Year   = {2025}
}
</pre>
      </div>
    </td>
  </tr>
















<!-- StarttPaper one -->
   <tr>
    <td width="33%" valign="top" align="center" style="padding-top:0px"><a href="https://visdomlab.github.io/EKS/"><img src="./vinod_files/wacv25_sfda.png" alt="sym" width="100%" style="padding:0px;border-radius:15px;border:0px solid black"></a></td>
        <td width="67%" valign="top" style="padding-top:10px">
      <p><a href="https://visdomlab.github.io/EKS/" id="wacv25_sfda_id">
         <!-- <img src="./vinod_files/new.png"  width="6%" style="border-style: none"> -->
  <heading>Label Calibration in Source Free Domain Adaptation</heading></a><br>
       Shivangi Rai, Rini Thakur, Kunal Jangid, <strong>Vinod K Kurmi</strong><br>
             <em>IEEE Winter Conference of Applications on Computer Vision (WACV)</em>, 2025.

      </p>
      <div class="paper" id="wacv25_sfda">
      <a href="javascript:toggleblock(&#39;wacv25sfda_abs&#39;)">abstract</a> |
        <a href="https://visdomlab.github.io/EKS/">webpage</a> |
      <a href="https://arxiv.org/pdf/2501.07072">ArXiv</a> |
      <!-- <a href="https://vinodkkurmi.github.io/">code</a> | -->
      <a shape="rect" href="javascript:togglebib(&#39;wacv25_sfda&#39;)" class="togglebib">bibtex</a> |
     

      <p align="justify"> <i id="wacv25sfda_abs" style="display: none;">Source-free domain adaptation (SFDA) utilizes a pre-trained source model with unlabeled target data. Self-supervised SFDA techniques generate pseudolabels from the pre-trained source model, but these pseudolabels often contain noise due to domain discrepancies between the source and target domains. Traditional self-supervised SFDA techniques rely on deterministic model predictions using the softmax function, leading to unreliable pseudolabels. In this work, we propose to introduce predictive uncertainty and softmax calibration for pseudolabel refinement using evidential deep learning. The Dirichlet prior is placed over the output of the target network to capture uncertainty using evidence with a single forward pass. Furthermore, softmax calibration solves the translation invariance problem to assist in learning with noisy labels. We incorporate a combination of evidential deep learning loss and information maximization loss with calibrated softmax in both prior and non-prior target knowledge SFDA settings. Extensive experimental analysis shows that our method outperforms other state-of-the-art methods on benchmark datasets.</i></p>

 <pre xml:space="preserve" style="display: none;">@inproceedings{rai_wacv25,
Author = {Rai Shivangi, 
and Thakur, Rini
and Jangid, Kunal
and Kurmi, Vinod  },
Title = {Label Calibration in
Source Free Domain Adaptation},
Booktitle = {WACV},
Year   = {2025}
}
</pre>
      </div>
    </td>
  </tr>





<!-- StarttPaper one -->
   <tr>
    <td width="33%" valign="top" align="center" style="padding-top:0px"><a href="https://vinodkkurmi.github.io/"><img src="./vinod_files/bmvc24_1.png" alt="sym" width="100%" style="padding:0px;border-radius:15px;border:0px solid black"></a></td>
        <td width="67%" valign="top" style="padding-top:10px">
      <p><a href="https://vinodkkurmi.github.io/" id="bmvc24_id">
         <!-- <img src="./vinod_files/new.png"  width="6%" style="border-style: none"> -->
  <heading>CosFairNet:A Parameter-Space based Approach for Bias Free Learning</heading></a><br>
       Rajeev Ranjan Dwivedi, Priyadarshini Kumari, <strong>Vinod K Kurmi</strong><br>
             <em>British Machine Vision Conference (BMVC)</em>, 2024.

      </p>
      <div class="paper" id="bmvc24">
      <a href="javascript:toggleblock(&#39;bmvc24_abs&#39;)">abstract</a> |
        <a href="https://visdomlab.github.io/CosFairNet/">webpage</a> |
      <a shape="rect" href="javascript:togglebib(&#39;bmvc24&#39;)" class="togglebib">bibtex</a> |
     

      <p align="justify"> <i id="bmvc24_abs" style="display: none;">Deep neural networks trained on biased data often inadvertently learn unintended inference rules, particularly when labels are strongly correlated with biased features. Existing bias mitigation methods typically involve either a) predefining bias types and enforcing them as prior knowledge or b) reweighting training samples to emphasize biasconflicting samples over bias-aligned samples. However, both strategies address bias indirectly in the feature or sample space, with no control over learned weights, making it difficult to control the bias propagation across different layers. Based on this observation, we introduce a novel approach to address bias directly in the model’s parameter space, preventing its propagation across layers. Our method involves training two models: a bias model for biased features and a debias model for unbiased details, guided by the bias model. We enforce dissimilarity in the debias model’s later layers and similarity in its initial layers with the bias model, ensuring it learns unbiased low-level features without adopting biased high-level abstractions. By incorporating this explicit constraint during training, our approach shows enhanced classification accuracy and debiasing effectiveness across various synthetic and real-world datasets of different sizes. Moreover, the proposed method demonstrates robustness across different bias types and percentages of biased samples in the training data</i></p>

 <pre xml:space="preserve" style="display: none;">@inproceedings{rajeevbmvc24_abs,
Author = {Dwivedi, Rajeev R
and Kumari, Priyadarshini
and Kurmi, Vinod  },
Title = {CosFairNet:A Parameter-Space based
Approach for Bias Free Learning},
Booktitle = {BMVC},
Year   = {2024}
}
</pre>
      </div>
    </td>
  </tr>

<!-- End paper  one -->









<!-- StarttPaper one -->
   <tr>
    <td width="33%" valign="top" align="center" style="padding-top:0px"><a href="https://vinodkkurmi.github.io/"><img src="./vinod_files/interspeech24.png" alt="sym" width="100%" style="padding:0px;border-radius:15px;border:0px solid black"></a></td>
        <td width="67%" valign="top" style="padding-top:10px">
      <p><a href="https://vinodkkurmi.github.io/" id="interspeech24_id">
         <!-- <img src="./vinod_files/new.png"  width="6%" style="border-style: none"> -->
  <heading>Towards Robust Few-shot Class Incremental Learning in Audio Classification using Contrastive Representation</heading></a><br>
       Riyansha Singh, Parinita Nema, <strong>Vinod K Kurmi</strong><br>
             <em>Conference of the International Speech Communication Association (Interspeech)</em>, 2024.

      </p>
      <div class="paper" id="interspeech24">
      <a href="javascript:toggleblock(&#39;interspeech24_abs&#39;)">abstract</a> |
        <a href="https://visdomlab.github.io/FsACLearning/">webpage</a> |
      <a href="https://arxiv.org/pdf/2407.19265">ArXiv</a> |
      <!-- <a href="https://vinodkkurmi.github.io/">code</a> | -->
      <a shape="rect" href="javascript:togglebib(&#39;interspeech24&#39;)" class="togglebib">bibtex</a> |
     

      <p align="justify"> <i id="interspeech24_abs" style="display: none;">In machine learning applications, gradual data ingress is common, especially in audio processing where incremental learning is vital for real-time analytics. Few-shot class-incremental learning addresses challenges arising from limited incoming data. Existing methods often integrate additional trainable components or rely on a fixed embedding extractor post-training on base sessions to mitigate concerns related to catastrophic forgetting and the dangers of model overfitting. However, using cross-entropy loss alone during base session training is suboptimal for audio data. To address this, we propose incorporating supervised contrastive learning to refine the representation space, enhancing discriminative power and leading to better generalization since it facilitates seamless integration of incremental classes, upon arrival. Experimental results on NSynth and LibriSpeech datasets with 100 classes, as well as ESC dataset with 50 and 10 classes, demonstrate state-of-the-art performance.</i></p>

 <pre xml:space="preserve" style="display: none;">@inproceedings{singh_intespeech24,
Author = {Singh Riyansha, 
and Nema, Parinita
and Kurmi, Vinod  },
Title = {Towards Robust Few-shot 
Class Incremental Learning in Audio
Classification using Contrastive Representation},
Booktitle = {Interspeech},
Year   = {2024}
}
</pre>
      </div>
    </td>
  </tr>





<!-- StarttPaper one -->
   <tr>
    <td width="33%" valign="top" align="center" style="padding-top:0px"><a href="https://vinodkkurmi.github.io/"><img src="./vinod_files/cikm24.png" alt="sym" width="100%" style="padding:0px;border-radius:15px;border:0px solid black"></a></td>
        <td width="67%" valign="top" style="padding-top:10px">
      <p><a href="https://vinodkkurmi.github.io/" id="cikm24_id">
         <!-- <img src="./vinod_files/new.png"  width="6%" style="border-style: none"> -->
  <heading>Quantifying Uncertainty in Neural Networks through Residuals</heading></a><br>
       Dalavai Udbhav Mallanna, Rajeev Ranjan Dwivedi, Rini Smita Thakur, <strong>Vinod K Kurmi</strong><br>
             <em>ACM International Conference on Information and Knowledge Management (CIKM)</em>, 2024.

      </p>
      <div class="paper" id="cikm24">
      <a href="javascript:toggleblock(&#39;cikm24_abs&#39;)">abstract</a> |
        <a href="https://visdomlab.github.io/HetGP/">webpage</a> |
      <a shape="rect" href="javascript:togglebib(&#39;cikm24&#39;)" class="togglebib">bibtex</a> |
     

      <p align="justify"> <i id="cikm24_abs" style="display: none;">Regression models are of fundamental importance in explicitly explaining the response variable in terms of covariates. However, point predictions of these models limit them from many real world applications. Heteroscedasticity is common in most real-world scenarios and is hard to model due to its randomness. The Gaussian process generally captures epistemic (model) uncertainty but fails to capture heteroscedastic aleatoric uncertainty. The framework of HetGP inherently captures both epistemic and aleatoric by placing independent GP’s priors on both mean function and error term. We propose the posthoc HetGP on the residuals of the trained deterministic neural network to obtain both epistemic and aleatoric uncertainty. The advantage of posthoc HetGP on residuals is that it can be extended to any type of model, since the model is assumed to be black-box that gives point predictions. We demonstrate our approach through simulation studies and UCI regression datasets.</i></p>

 <pre xml:space="preserve" style="display: none;">@inproceedings{udbhav,
Author = {Udbhav Mallanna,Dalavai 
and Dwivedi, Rajeev R
and Thakur, Rini S
and Kurmi, Vinod  },
Title = {Quantifying Uncertainty
in Neural Networks through Residuals},
Booktitle = {CIKM},
Year   = {2024}
}
</pre>
      </div>
    </td>
  </tr>
























<!-- StarttPaper one -->
   <tr>
    <td width="33%" valign="top" align="center" style="padding-top:0px"><a href="https://vinodkkurmi.github.io/"><img src="./vinod_files/ivc_3.jpg" alt="sym" width="100%" style="padding:0px;border-radius:15px;border:0px solid black"></a></td>
        <td width="67%" valign="top" style="padding-top:10px">
      <p><a href="https://vinodkkurmi.github.io/" id="ivc3_id">
         <!-- <img src="./vinod_files/new.png"  width="6%" style="border-style: none"> -->
  <heading>Distilling Knowledge for Occlusion Robust Monocular 3D Face Reconstruction</heading></a><br>
       H. Tiwari, <strong>Vinod K Kurmi</strong>, Venkatesh K Subramanian, Yong-Sheng Chen<br>
      <em>Image and Vision Computing, (IMAVIS)</em>, 2023.
      </p>
      <div class="paper" id="ivc3">
      <a href="javascript:toggleblock(&#39;ivc3_abs&#39;)">abstract</a> |
      <a href="https://www.sciencedirect.com/science/article/abs/pii/S0262885623001373">pdf</a> |
      <a shape="rect" href="javascript:togglebib(&#39;ivc3&#39;)" class="togglebib">bibtex</a> |
     

      <p align="justify"> <i id="ivc3_abs" style="display: none;">Recently, there have been significant advancements in the 3D face reconstruction field, largely driven by monocular image-based deep learning methods. However, these methods still face challenges in reliable deployments due to their sensitivity to facial occlusions and inability to maintain identity consistency across different occlusions within the same facial image. To address these issues, we propose two frameworks: Distillation Assisted Mono Image Occlusion Robustification (DAMIOR) and Duplicate Images Assisted Multi Occlusions Robustification (DIAMOR). The DAMIOR framework leverages the knowledge from the Occlusion Frail Trainer (OFT) network to enhance robustness against facial occlusions. Our proposed method overcomes the sensitivity to occlusions and improves reconstruction accuracy. To tackle the issue of identity inconsistency, the DIAMOR framework utilizes the estimates from DAMIOR to mitigate inconsistencies in geometry and texture, collectively known as identity, of the reconstructed 3D faces. We evaluate the performance of DAMIOR on two variations of the CelebA test dataset: empirical occlusions and irrational occlusions. Furthermore, we analyze the performance of the proposed DIAMOR framework using the irrational occlusion-based variant of the CelebA test dataset. Our methods outperform state-of-the-art approaches by a significant margin. For example, DAMIOR reduces the 3D vertex-based shape error by 41.1% and the texture error by 21.8% for empirical occlusions. Besides, for facial data with irrational occlusions, DIAMOR achieves a substantial decrease in shape error by 42.5% and texture error by 30.5%. These results demonstrate the effectiveness of our proposed methods. </i></p>

 <pre xml:space="preserve" style="display: none;">@inproceedings{hitika_ivc3,
Author = {Tiwari, Hitika
and Kurmi, Vinod K
and  Subramanian,
Venkatesh K and
Chen, Yong Sheng },
Title = {Distilling Knowledge
for Occlusion Robust Monocular
3D Face Reconstruction},
Booktitle = {InterSpeech},
Year   = {2022}
}
</pre>
      </div>
    </td>
  </tr>

<!-- End paper  one -->




<!-- StarttPaper one -->
   <tr>
    <td width="33%" valign="top" align="center" style="padding-top:10px"><a href="https://vinodkkurmi.github.io/"><img src="./vinod_files/intersp.jpg" alt="sym" width="110%" style="padding:10px;border-radius:15px;border:0px solid black"></a></td>
        <td width="67%" valign="top" style="padding-top:10px">
      <p><a href="https://vinodkkurmi.github.io/" id="interspeech22_id">
         <!-- <img src="./vinod_files/new.png"  width="6%" style="border-style: none"> -->
  <heading>Generalized Keyword Spotting using ASR embeddings</heading></a><br>
       Kirandevraj R, <strong>Vinod K Kurmi</strong>, Vinay P Namboodiri, C V Jawahar<br>
       <em>Conference of the International Speech Communication Association (Interspeech) </em> 2022, Incheon Korea.
      </p>
      <div class="paper" id="interspeech22">
      <a href="javascript:toggleblock(&#39;interspeech22_abs&#39;)">abstract</a> |
       <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2022/r22_interspeech.pdf">pdf</a> |
      <a shape="rect" href="javascript:togglebib(&#39;interspeech22&#39;)" class="togglebib">bibtex</a> |
     

      <p align="justify"> <i id="interspeech22_abs" style="display: none;">Keyword Spotting (KWS) detects a set of pre-defined spoken keywords. Building a KWS system for an arbitrary set re- quires massive training datasets. We propose to use the text transcripts from an Automatic Speech Recognition (ASR) sys- tem alongside triplets for KWS training. The intermediate rep- resentation from the ASR system trained on a speech corpus is used as acoustic word embeddings for keywords. Triplet loss is added to the Connectionist Temporal Classification (CTC) loss in the ASR while training. This method achieves an Average Precision (AP) of 0.843 over 344 words unseen by the model trained on the TIMIT dataset. In contrast, the Multi-View re- current method that learns jointly on the text and acoustic em- beddings achieves only 0.218 for out-of-vocabulary words. This method is also applied to low-resource languages such as Tamil by converting Tamil characters to English using transliteration. This is a very challenging novel task for which we provide a dataset of transcripts for the keywords. Despite our model not generalizing well, we achieve a benchmark AP of 0.321 on over 38 words unseen by the model on the MSWC Tamil keyword set. The model also produces an accuracy of 96.2% for classifi- cation tasks on the Google Speech Commands dataset. </i></p>

 <pre xml:space="preserve" style="display: none;">@inproceedings{kiran_inter22,
Author = {R,Kiran.
and Kurmi, Vinod K
and Namboodiri, Vinay P
and Jawhar, CV},
Title = {Generalized Keyword
Spotting using ASR embeddings},
Booktitle = {InterSpeech},
Year   = {2022}
}
</pre>
      </div>
    </td>
  </tr>

<!-- End paper  one -->





<!-- StarttPaper one -->
   <tr>
    <td width="33%" valign="top" align="center" style="padding-top:10px"><a href="https://vinodkkurmi.github.io/"><img src="./vinod_files/aaai22.gif" alt="sym" width="110%" style="padding:10px;border-radius:15px;border:0px solid black"></a></td>
        <td width="67%" valign="top" style="padding-top:10px">
      <p><a href="https://vinodkkurmi.github.io/" id="aaai22_id">
         <!-- <img src="./vinod_files/new.png"  width="6%" style="border-style: none"> -->
  <heading>Gradient Based Activations for Accurate Bias-Free Learning</heading></a><br>
       <strong>Vinod K Kurmi*</strong>, Rishabh Sharma*, Yash Vardhan Sharma*, Vinay P Namboodiri (*equal contributions)<br>
       <em>Proceedings of the AAAI Conference on Artificial Intelligence, (AAAI), Vancouver BC, Canada,</em> 2022.
      </p>
      <div class="paper" id="aaai22">
      <a href="https://vinodkkurmi.github.io/GBA">webpage</a> |
      <a href="https://arxiv.org/pdf/2202.10943.pdf">pdf</a> |
      <a href="javascript:toggleblock(&#39;aaai22_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;aaai22&#39;)" class="togglebib">bibtex</a> |
      <a href="https://github.com/vinodkkurmi/GBA">code</a> |

      <p align="justify"> <i id="aaai22_abs" style="display: none;">Bias mitigation in machine learning models is imperative, yet challenging. While several approaches have been proposed, one view towards mitigating bias is through adversarial learning. A discriminator is used to identify the bias attributes such as gender, age or race in question.  This discriminator is used adversarially to ensure that it cannot distinguish the bias attributes.  The main drawback in such a model is that it directly introduces a trade-off with accuracy as the features that the discriminator deems to be sensitive for discrimination of bias could be correlated with classification. In this work we solve the problem. We show that a biased discriminator can actually be used to improve this bias-accuracy tradeoff. Specifically, this is achieved by using a feature masking approach using the discriminator's gradients. We ensure that the features favoured for the bias discrimination are de-emphasized and the unbiased features are enhanced during classification. We show that this simple approach works well to reduce bias as well as improve accuracy significantly. We evaluate the proposed model on standard benchmarks. We improve the accuracy of the adversarial methods while maintaining or even improving the unbiasness and also outperform several other recent methods. </i></p>

 <pre xml:space="preserve" style="display: none;">@inproceedings{kurmi_aaai22,
Author = {Kurmi,Vinod K.
and Sharma, Rishabh and
Sharma, Yash Vardhan and
Namboodiri, Vinay P},
Title = {Gradient Based
Activations for Accurate
Bias-Free Learning},
Booktitle = {AAAI},
Year   = {2022}
}
</pre>
      </div>
    </td>
  </tr>

<!-- End paper  one -->


<!-- uncommet from here  -->


<!-- StarttPaper one -->

   <tr>
    <td width="30%" valign="top" align="center" style="padding-top:10px"><a href="https://vinodkkurmi.github.io/"><img src="./vinod_files/wacv22.png" alt="sym" width="80%" style="padding:10px;border-radius:10px;border:0px solid black"></a></td>
        <td width="50%" valign="top" style="padding-top:10px">
      <p><a href="https://vinodkkurmi.github.io/" id="wacv22_id">
         <!-- <img src="./vinod_files/new.png"  width="6%" style="border-style: none"> -->
  <heading>Occlusion Resistant Network for 3D Face Reconstruction</heading></a><br>
      H. Tiwari, <strong>Vinod K Kurmi</strong>, Venkatesh K Subramanian, Yong-Sheng Chen<br>
       <em>IEEE Winter Conference of Applications on Computer Vision (WACV)</em>, 2022.
      </p>

      <div class="paper" id="wacv22">
      <a href="https://openaccess.thecvf.com/content/WACV2022/html/Tiwari_Occlusion_Resistant_Network_for_3D_Face_Reconstruction_WACV_2022_paper.html">pdf</a> |
      <a href="javascript:toggleblock(&#39;wacv22_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;wacv22&#39;)" class="togglebib">bibtex</a> |
      <!-- <a href="https://vinodkkurmi.github.io/">code</a> | -->

      <p align="justify"> <i id="wacv22_abs" style="display: none;">3D face reconstruction from a monocular face image is a mathematically ill-posed problem. Recently, we observed a surge of interest in deep learning-based approaches to address the issue. These methods possess extreme sensitivity towards occlusions. Thus, in this paper, we present a novel context-learning-based distillation approach to tackle the occlusions in the face images. Our training pipeline focuses on distilling the knowledge from a pre-trained occlusion-sensitive deep network. The proposed model learns the context of the target occluded face image. Hence our approach uses a weak model (unsuitable for occluded face images) to train a highly robust network towards partially and fully-occluded face images. We obtain a landmark accuracy of $0.77$ against $5.84$ of recent state-of-the-art-method for real-life challenging facial occlusions. Also, we propose a novel end-to-end training pipeline to reconstruct 3D faces from multiple variations of the target image per identity to emphasize the significance of visible facial features during learning. For this purpose, we leverage a novel composite multi-occlusion loss function. Our multi-occlusion per identity model shows a dip in the landmark error by a large margin of $6.67$ in comparison to a recent state-of-the-art method. We deploy the occluded variations of the CelebA validation dataset and AFLW2000-3D face dataset: naturally-occluded and artificially occluded, for the comparisons. We comprehensively compare our results with the other approaches concerning the accuracy of the reconstructed 3D face mesh for occluded face images.. </i></p>

 <pre xml:space="preserve" style="display: none;">@inproceedings{tiwati_wacv22,
Author = {Tiwari, H.
and Kurmi,Vinod Kumar
and Venkatesh, KS and
Chen,Yong-Sheng},
Title = {Occlusion Resistant Network
 for 3D Face Reconstruction},
Booktitle = {WACV},
Year   = {2022}
}
<!-- </pre>
      </div>
    </td>
  </tr> -->
<!-- to here -->
<!-- End paper  one -->

<!-- StarttPaper one -->

   <tr>
    <td width="33%" valign="top" align="center" style="padding-top:10px"><a href="https://vinodkkurmi.github.io/"><img src="./vinod_files/ivc_2.png" alt="sym" width="100%" style="padding:10px;border-radius:15px;border:0px solid black"></a></td>
        <td width="67%" valign="top" style="padding-top:10px">
      <p><a href="https://vinodkkurmi.github.io/" id="ivc_2_id">
         <!-- <img src="./vinod_files/new.png"  width="6%" style="border-style: none"> -->
  <heading>MUMC: Minimizing Uncertainty of Mixture of Cues</heading></a><br>
      Badri N Patro, <strong>Vinod K Kurmi</strong>, Sandeep Kumar,  Vinay P Namboodiri<br>
       <em>Image and Vision Computing,(IMAVIS)</em>, 2021.
      </p>

      <div class="paper" id="ivc_2">
      <!-- <a href="https://vinodkkurmi.github.io/">webpage</a> | -->
      <a href="https://www.sciencedirect.com/science/article/abs/pii/S0262885621001852">pdf</a> |
      <a href="javascript:toggleblock(&#39;ivc_2_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;ivc_2 &#39;)" class="togglebib">bibtex</a> |
      <!-- <a href="https://vinodkkurmi.github.io/">code</a> | -->

      <p align="justify"> <i id="ivc_2_abs" style="display: none;">Generating natural questions from an image is a semantic task that requires using
vision and language modalities to learn multimodal representations. Images can have
multiple visual and language cues such as places, captions, and tags. In this paper,
we propose a principled deep Bayesian learning framework that combines these cues
to produce natural questions. We observe that with the addition of more cues and
by minimizing uncertainty in the among cues, the Bayesian network becomes more
confident. We propose a Minimizing Uncertainty of Mixture of Cues (MUMC), that
minimizes uncertainty present in a mixture of cues experts for generating probabilistic
questions. This is a Bayesian framework and the results show a remarkable similarity to
natural questions as validated by a human study. Ablation studies of our model indicate
that a subset of cues is inferior at this task and hence the principled fusion of cues is
preferred. Further, we observe that the proposed approach substantially improves over
state-of-the-art benchmarks on the quantitative metrics (BLEU-n, METEOR, ROUGE,
and CIDEr). </i></p>

 <pre xml:space="preserve" style="display: none;">@inproceedings{patro_ivc_2,
Author = {Patro, Badri Narayana
and Kumar, Sandeep and Kurmi,
Vinod Kumar and Namboodiri, Vinay},
Title = {MUMC: Minimizing Uncertainty
of Mixture of Cues},
Booktitle = {Image and Vision 
Computing,(IMAVIS)},
Year   = {2021}
}
</pre>
      </div>
    </td>
  </tr>

<!-- End paper  one -->
  <!-- StarttPaper one -->

   <tr>
    <td width="33%" valign="top" align="center" style="padding-top:10px"><a href="https://vinodkkurmi.github.io/"><img src="./vinod_files/neuro_2021.png" alt="sym" width="100%" style="padding:10px;border-radius:15px;border:0px solid black"></a></td>
        <td width="67%" valign="top" style="padding-top:10px">
      <p><a href="https://vinodkkurmi.github.io/" id="idda_j_1">
        <!--  <img src="./vinod_files/new.png"  width="6%" style="border-style: none"> -->
  <heading>Exploring Dropout Discriminator for Domain Adaptation</heading></a><br>
      <strong>Vinod K Kurmi</strong>,  Venkatesh K Subramanian,  Vinay P Namboodiri<br>
       <em>NeuroComputing(Journal-Elsevier)</em>, 2021.
      </p>

      <div class="paper" id="vinod_cd3a_j">
      <a href="https://vinodkkurmi.github.io/">webpage</a> |
      <a href="https://www.sciencedirect.com/science/article/abs/pii/S092523122100967X">pdf</a> |
      <a href="javascript:toggleblock(&#39;vinod_cd3a_j_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;vinod_cd3a_j&#39;)" class="togglebib">bibtex</a> |
      <a href="https://vinodkkurmi.github.io/">code</a> |

      <p align="justify"> <i id="vinod_cd3a_j_abs" style="display: none;">Adaptation of a classifier to new domains is one of the challenging problems in machine learning. This has been addressed using many deep and non-deep learning based methods. Among the methodologies used, that of adversarial learning is widely applied to solve many deep learning problems along with domain adaptation. These methods are based on a discriminator that ensures source and target distributions are close.  However, here we suggest that rather than using a point estimate obtaining by a single discriminator, it would be useful if a distribution based on ensembles of discriminators could be used to bridge this gap. This could be achieved using multiple classifiers or using  traditional ensemble methods.} In contrast, we suggest that a Monte Carlo dropout based ensemble discriminator could suffice to obtain the distribution based discriminator. Specifically, we propose a curriculum based dropout discriminator that gradually increases the variance of the sample based distribution and the corresponding reverse gradients are used to align the source and target feature representations. </i></p>

 <pre xml:space="preserve" style="display: none;">@inproceedings{kurmi2021_idda_j,
Author = {Kurmi, Vinod Kumar and K S ,
Venaktesh and  Namboodiri, Vinay},
Title = {Exploring Dropout 
Discriminator for Domain Adaptation},
Booktitle = {Neurocomputing},
Year   = {2021}
}
</pre>
      </div>
    </td>
  </tr>

<!-- End paper  one -->


  <!-- StarttPaper one -->

   <tr>
    <td width="33%" valign="top" align="center" style="padding-top:10px"><a href="https://vinodkkurmi.github.io/"><img src="./vinod_files/shanu.png" alt="sym" width="100%" style="padding:10px;border-radius:15px;border:0px solid black"></a></td>
        <td width="67%" valign="top" style="padding-top:10px">
      <p><a href="https://vinodkkurmi.github.io/" id="shanu">
         <!-- <img src="./vinod_files/new.png"  width="6%" style="border-style: none"> -->
  <heading>Mitigating Uncertainty of Classifier for Unsupervised Domain Adaptation</heading></a><br>
      Shanu Kumar, <strong>Vinod K Kurmi</strong>,  Praphul Singh,  Vinay P Namboodiri<br>
       <em>Preprint- arXiv</em>, 2021.
      </p>

      <div class="paper" id="shanu">
      <a href="https://arxiv.org/abs/2107.00727">pdf</a> |
      <a href="javascript:toggleblock(&#39;shanu_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;shanu&#39;)" class="togglebib">bibtex</a> |
      

      <p align="justify"> <i id="shanu_abs" style="display: none;">Understanding unsupervised domain adaptation has been an important task that has been well explored. However, the wide variety of methods have not analyzed the role of a classifier's performance in detail. In this paper, we thoroughly examine the role of a classifier in terms of matching source and target distributions. We specifically investigate the classifier ability by matching a) the distribution of features, b) probabilistic uncertainty for samples and c) certainty activation mappings. Our analysis suggests that using these three distributions does result in a consistently improved performance on all the datasets. Our work thus extends present knowledge on the role of the various distributions obtained from the classifier towards solving unsupervised domain adaptation. </i></p>

 <pre xml:space="preserve" style="display: none;">@inproceedings{shanu,
Author = {Kumar, Shanu and
Kurmi, Vinod Kumar and Singh,
Praphul and Namboodiri, Vinay P},
Title = {Mitigating Uncertainty
 of Classifier for Unsupervised Domain Adaptation},
Booktitle = {Preprint- arXiv},
Year   = {2021}
}
</pre>
      </div>
    </td>
  </tr>

<!-- End paper  one -->

  <!-- StarttPaper one -->

   <tr>
    <td width="33%" valign="top" align="center" style="padding-top:10px"><a href="https://vinodkkurmi.github.io/"><img src="./vinod_files/blessen.png" alt="sym" width="70%" style="padding:10px;border-radius:15px;border:0px solid black"></a></td>
        <td width="67%" valign="top" style="padding-top:10px">
      <p><a href="https://vinodkkurmi.github.io/" id="blessen">
        <!--  <img src="./vinod_files/new.png"  width="6%" style="border-style: none"> -->
  <heading>Prb-GAN: A Probabilistic Framework for GAN Modelling
</heading></a><br>
      Blessen George, <strong>Vinod K Kurmi</strong>,Vinay P Namboodiri<br>
       <em>Preprint- ArXiv</em>, 2021.
      </p>

      <div class="paper" id="blessen">
      <a href="https://arxiv.org/abs/2107.05241">pdf</a> |
      <a href="javascript:toggleblock(&#39;blessen_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;blessen&#39;)" class="togglebib">bibtex</a> |
      <a href="https://github.com/blessengeorge/mygan">code</a> |

      <p align="justify"> <i id="blessen_abs" style="display: none;">Generative adversarial networks (GANs) are very popular to generate realistic images, but they often suffer from the training instability issues and the phenomenon of mode loss. In order to attain greater diversity in GAN synthesized data, it is critical to solving the problem of mode loss. Our work explores probabilistic approaches to GAN modelling that could allow us to tackle these issues. We present Prb-GANs, a new variation that uses dropout to create a distribution over the network parameters with the posterior learnt using variational inference. We describe theoretically and validate experimentally using simple and complex datasets the benefits of such an approach. We look into further improvements using the concept of uncertainty measures. Through a set of further modifications to the loss functions for each network of the GAN, we are able to get results that show the improvement of GAN performance. Our methods are extremely simple and require very little modification to existing GAN architecture. </i></p>
 <pre xml:space="preserve" style="display: none;">
@inproceedings{george2021prb,
  title={Prb-GAN: A Probabilistic
  Framework for GAN Modelling},
  author={George, Blessen and 
  Kurmi, Vinod K and Namboodiri, Vinay P},
  journal={arXiv preprint arXiv:2107.05241},
  year={2021}
}
</pre>
      </div>
    </td>
  </tr>

<!-- End paper  one -->



      <tr>
    <td width="33%" valign="top" align="center" style="padding-top:10px"><a href="https://vinodkkurmi.github.io/"><img src="./vinod_files/ijcnn_1.png" alt="sym" width="100%" style="padding:10px;border-radius:15px;border:0px solid black"></a></td>
        <td width="67%" valign="top" style="padding-top:10px">
      <p><a href="https://vinodkkurmi.github.io/" id="ijcnn_1">
        <!--  <img src="./vinod_files/new.png"  width="6%" style="border-style: none"> -->
  <heading>Sensor-invariant Fingerprint ROI SegmentationUsing Recurrent Adversarial Learning</heading></a><br>
        Indu Joshi, Ayush Utkarsh, Riya Kothari, <strong>Vinod K Kurmi</strong>, Antitza Dantcheva, Sumantra Dutta Roy, Prem Kumar Kalra  <br>
       <em>International Joint Conference on Neural Networks (IJCNN)</em>, 2021.
      </p>

      <div class="paper" id="vinod_ijcnn21_1">
      <a href="https://vinodkkurmi.github.io/">pdf</a> |
      <a href="javascript:toggleblock(&#39;ijcnn21_1_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;vinod_ijcnn21_1&#39;)" class="togglebib">bibtex</a> |
      <a href="https://vinodkkurmi.github.io/">arXiv</a> |
    <p align="justify"> <i id="ijcnn21_1_abs" style="display: none;">A fingerprint region of interest (roi) segmentation algorithm is designed to separate the foreground fingerprint from the background noise. All the learning based state-of-the-art fingerprint roi segmentation algorithms proposed in the literature are benchmarked on scenarios when both training and testing databases consist of fingerprint images acquired from the same sensors. However, when testing is conducted on a different sensor, the segmentation performance obtained is often unsatisfactory. As a result, every time a new fingerprint sensor is used for testing, the fingerprint roi segmentation model needs to be re-trained with the fingerprint image acquired from the new sensor and its corresponding manually marked ROI. Manually marking fingerprint ROI is expensive because firstly, it is time consuming and more importantly, requires domain expertise. In order to save the human effort in generating annotations required by state-of-the-art, we propose a fingerprint roi segmentation model which aligns the features of fingerprint images derived from the unseen sensor such that they are similar to the ones obtained from the fingerprints whose ground truth roi masks are available for training. Specifically, we propose a recurrent adversarial learning based feature alignment network that helps the fingerprint roi segmentation model to learn sensor-invariant features. Consequently, sensor-invariant features learnt by the proposed roi segmentation model help it to achieve improved segmentation performance on fingerprints acquired from the new sensor. Experiments on publicly available FVC databases demonstrate the efficacy of the proposed work.</i></p>
<!--       <p align="justify"> <i id="icassp21_abs" style="display: none;">Unsupervised Domain adaptation methods solve the adaptation problem for an unlabeled target set, assuming that the source dataset is available with all labels. However, the availability of actual source samples is not always possible in practical cases. It could be due to memory constraints, privacy concerns, and challenges in sharing data. This practical scenario creates a bottleneck in the domain adaptation problem. This paper addresses this challenging scenario by proposing a domain adaptation technique that does not need any source data. Instead of the source data, we are only provided with a classifier that is trained on the source data. Our proposed approach is based on a generative framework, where the trained classifier is used for generating samples from the source classes. We learn the joint distribution of data by using the energy-based modeling of the trained classifier. At the same time, a new classifier is also adapted for the target domain. We perform various ablation analysis under different experimental setups and demonstrate that the proposed approach achieves better results than the baseline models in this extremely novel scenario.</i></p> -->

 <pre xml:space="preserve" style="display: none;">@inproceedings{joshi021_1,
Author={Joshi, Indu and Kothari, Riya
 and Utkarsh, Ayush and Kurmi, Vinod K
 and Dantcheva,Antitza and Roy,
 Sumantra Dutta and Kalra, Prem Kumar},
Title = {Sensor-invariant Fingerprint
ROI SegmentationUsing Recurrent 
Adversarial Learning},
Booktitle = {IJCNN},
Year   = {2021}
}
</pre>
      </div>
    </td>
  </tr>



<!-- StarttPaper  -->
      <tr>
    <td width="33%" valign="top" align="center" style="padding-top:10px"><a href="https://vinodkkurmi.github.io/"><img src="./vinod_files/ijcnn_2.png" alt="sym" width="100%" style="padding:10px;border-radius:15px;border:0px solid black"></a></td>
        <td width="67%" valign="top" style="padding-top:10px">
      <p><a href="https://vinodkkurmi.github.io/" id="ICASSP21_2">
        <!--  <img src="./vinod_files/new.png"  width="6%" style="border-style: none"> -->
  <heading>Data Uncertainty Guided Noise-awarePreprocessing Of Fingerprints</heading></a><br>
        Indu Joshi, Ayush Utkarsh, Riya Kothari, <strong>Vinod K Kurmi</strong>, Antitza Dantcheva, Sumantra Dutta Roy, Prem Kumar Kalra  <br>
       <em>International Joint Conference on Neural Networks (IJCNN)</em>, 2021.
      </p>

      <div class="paper" id="vinod_ijcnn21_2">
      <a href="https://vinodkkurmi.github.io/">pdf</a> |
      <a href="javascript:toggleblock(&#39;ijcnn21_2_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;vinod_ijcnn21_2&#39;)" class="togglebib">bibtex</a> |
      <a href="https://vinodkkurmi.github.io/">arXiv</a> |
    <p align="justify"> <i id="ijcnn21_2_abs" style="display: none;">The effectiveness of fingerprint-based authentication systems on good quality fingerprints is established long back. However, the performance of standard fingerprint matching systems on noisy and poor quality fingerprints is far from satisfactory. Towards this, we propose a data uncertainty-based framework which enables the state-of-the-art fingerprint preprocessing models to quantify noise present in the input image and identify fingerprint regions with background noise and poor ridge clarity. Quantification of noise helps the model two folds: firstly, it makes the objective function adaptive to the noise in a particular input fingerprint and consequently, helps to achieve robust performance on noisy and distorted fingerprint regions. Secondly, it provides a noise variance map which indicates noisy pixels in the input fingerprint image. The predicted noise variance map enables the end-users to understand erroneous predictions due to noise present in the input image. Extensive experimental evaluation on 13 publicly available fingerprint databases, across different architectural choices and two fingerprint processing tasks demonstrate effectiveness of the proposed framework.</i></p>
<!--       <p align="justify"> <i id="icassp21_abs" style="display: none;">Unsupervised Domain adaptation methods solve the adaptation problem for an unlabeled target set, assuming that the source dataset is available with all labels. However, the availability of actual source samples is not always possible in practical cases. It could be due to memory constraints, privacy concerns, and challenges in sharing data. This practical scenario creates a bottleneck in the domain adaptation problem. This paper addresses this challenging scenario by proposing a domain adaptation technique that does not need any source data. Instead of the source data, we are only provided with a classifier that is trained on the source data. Our proposed approach is based on a generative framework, where the trained classifier is used for generating samples from the source classes. We learn the joint distribution of data by using the energy-based modeling of the trained classifier. At the same time, a new classifier is also adapted for the target domain. We perform various ablation analysis under different experimental setups and demonstrate that the proposed approach achieves better results than the baseline models in this extremely novel scenario.</i></p> -->

 <pre xml:space="preserve" style="display: none;">@inproceedings{joshi021_2,
Author={Joshi, Indu and Kothari, Riya
 and Utkarsh, Ayush and Kurmi, Vinod K
 and Dantcheva,Antitza and Roy,
 Sumantra Dutta and Kalra, Prem Kumar},
Title = {Data Uncertainty Guided 
Noise-awarePreprocessing Of Fingerprints},
Booktitle = {IJCNN},
Year   = {2021}
}
</pre>
      </div>
    </td>
  </tr>
<!-- End paper   -->

<!-- StarttPaper one -->

   <tr>
    <td width="33%" valign="top" align="center" style="padding-top:10px"><a href="https://vinodkkurmi.github.io/DiscriminatorDomainAdaptation/"><img src="./vinod_files/intro_idda.png" alt="sym" width="90%" style="padding:10px;border-radius:15px;border:0px solid black"></a></td>
        <td width="67%" valign="top" style="padding-top:10px">
      <p><a href="https://vinodkkurmi.github.io/DiscriminatorDomainAdaptation/" id="idda_j_1">
        <!--  <img src="./vinod_files/new.png"  width="6%" style="border-style: none"> -->
  <heading>Informative Discriminator for Domain Adaptation</heading></a><br>
      <strong>Vinod K Kurmi</strong>,  Venkatesh K Subramanian,  Vinay P Namboodiri<br>
       <em>Image and Vision Computing,(IMAVIS)</em>, 2021.
      </p>

      <div class="paper" id="vinod_idda_j">
      <a href="https://vinodkkurmi.github.io/DiscriminatorDomainAdaptation/">webpage</a> |
      <a href="https://www.sciencedirect.com/science/article/abs/pii/S0262885621000858">pdf</a> |
      <a href="javascript:toggleblock(&#39;vinod_idda_j_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;vinod_idda_j&#39;)" class="togglebib">bibtex</a> |
      <a href="https://vinodkkurmi.github.io/DiscriminatorDomainAdaptation/">code</a> |

      <p align="justify"> <i id="vinod_idda_j_abs" style="display: none;">In this paper, we consider the problem of domain adaptation for multi-class classification, where we are provided a labeled set of examples in a source dataset and target dataset with no supervision. We tackle the mode collapse problem in adapting the classifier across domains. In this setting, we propose an adversarial learning-based approach using an informative discriminator. Our observation relies on the analysis that shows if the discriminator has access to all the information available, including the class structure present in the source dataset, then it can guide the transformation of features of the target set of classes to a more structured adapted space. Further, by training the informative discriminator using the more robust source samples, we are able to obtain better domain invariant features.  Using this formulation, we achieve state-of-the-art results for the standard evaluation on benchmark datasets. We also provide detailed analysis, which shows that using all the labeled information results in an improved domain adaptation.</i></p>

 <pre xml:space="preserve" style="display: none;">@inproceedings{kurmi2021_idda_j,
Author = {Kurmi, Vinod Kumar and K S ,
Venaktesh and  Namboodiri, Vinay},
Title = {Informative Discriminator
 for Domain Adaptation},
Booktitle = {Image and Vision Computing,(IMAVIS)},
Year   = {2021}
}
</pre>
      </div>
    </td>
  </tr>

<!-- End paper  one -->





<!-- StarttPaper  -->
      <tr>
    <td width="33%" valign="top" align="center" style="padding-top:10px"><a href="https://delta-lab-iitk.github.io/AVG/"><img src="./vinod_files/icassp2021.png" alt="sym" width="80%" style="padding:10px;border-radius:15px;border:0px solid black"></a></td>
        <td width="67%" valign="top" style="padding-top:10px">
      <p><a href="https://delta-lab-iitk.github.io/AVG/" id="ICASSP21_1">
  <heading>Collaborative Learning to Generate Audio-Video Jointly</heading></a><br>
      <strong>Vinod K Kurmi</strong>,  Vipul Bajaj, Badri N. Patro, Venkatesh K Subramanian, Vinay P. Namboodiri, Preethi Jyothi <br>
       <em>IEEE International Conference on Acoustics, Speech, and Signal Processing.(ICASSP)</em>, 2021.
      </p>

      <div class="paper" id="vinod_icassp">
      <a href="https://delta-lab-iitk.github.io/AVG/">webpage</a> |
      <a href="https://delta-lab-iitk.github.io/AVG/">pdf</a> |
      <a href="javascript:toggleblock(&#39;icassp21_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;vinod_icassp&#39;)" class="togglebib">bibtex</a> |
      <a href="https://delta-lab-iitk.github.io/AVG/">arXiv</a> |
      <a href="https://delta-lab-iitk.github.io/AVG/">code</a> |
    <p align="justify"> <i id="icassp21_abs" style="display: none;">There have been a number of techniques that have demonstrated the generation of multimedia data for one modality at a time using GANs, such as the ability to generate images, videos, and audio. However, so far, the task of multi-modal generation of data, specifically for audio and videos both, has not been sufficiently well-explored. Towards this, we propose a method that demonstrates that we are able to generate naturalistic samples of video and audio data by the joint correlated generation of audio and video modalities. The proposed method uses multiple discriminators to ensure that the audio, video, and the joint output are also indistinguishable from real-world samples. We present a dataset for this task and show that we are able to generate realistic samples. This method is validated using various standard metrics such as Inception Score, Frechet Inception Distance (FID) and through human evaluation.</i></p>
<!--       <p align="justify"> <i id="icassp21_abs" style="display: none;">Unsupervised Domain adaptation methods solve the adaptation problem for an unlabeled target set, assuming that the source dataset is available with all labels. However, the availability of actual source samples is not always possible in practical cases. It could be due to memory constraints, privacy concerns, and challenges in sharing data. This practical scenario creates a bottleneck in the domain adaptation problem. This paper addresses this challenging scenario by proposing a domain adaptation technique that does not need any source data. Instead of the source data, we are only provided with a classifier that is trained on the source data. Our proposed approach is based on a generative framework, where the trained classifier is used for generating samples from the source classes. We learn the joint distribution of data by using the energy-based modeling of the trained classifier. At the same time, a new classifier is also adapted for the target domain. We perform various ablation analysis under different experimental setups and demonstrate that the proposed approach achieves better results than the baseline models in this extremely novel scenario.</i></p> -->

 <pre xml:space="preserve" style="display: none;">@inproceedings{kurmi2021_avg,
Author = {Kurmi, Vinod Kumar and Bajaj, Vipul and Patro, Badri N and K S ,
Venaktesh and  Namboodiri, Vinay and Jyothi, Preethi},
Title = {Collaborative Learning to Generate Audio-Video Jointly},
Booktitle = {ICASSP},
Year   = {2021}
}
</pre>
      </div>
    </td>
  </tr>
<!-- End paper   -->

   <tr>
    <td width="33%" valign="top" align="center" style="padding-top:10px"><a href="https://delta-lab-iitk.github.io/SFDA/"><img src="./vinod_files/wacv2021_sdda.png" alt="sym" width="100%" style="padding:10px;border-radius:15px;border:0px solid black"></a></td>
        <td width="67%" valign="top" style="padding-top:10px">
      <p><a href="https://delta-lab-iitk.github.io/SFDA/" id="WACV21_1">
<!--          <img src="./vinod_files/new.png"  width="6%" style="border-style: none"> -->
  <heading>Domain Impression: A Source Data Free Domain Adaptation Method</heading></a><br>
      <strong>Vinod K Kurmi</strong>,  Venkatesh K Subramanian,  Vinay P Namboodiri<br>
       <em>IEEE Winter Conference of Applications on Computer Vision (WACV)</em>, 2021.
      </p>

      <div class="paper" id="vinod_sfda">
      <a href="https://delta-lab-iitk.github.io/SFDA/">webpage</a> |
      <a href="https://openaccess.thecvf.com/content/WACV2021/papers/Kurmi_Domain_Impression_A_Source_Data_Free_Domain_Adaptation_Method_WACV_2021_paper.pdf">pdf</a> |
      <a href="javascript:toggleblock(&#39;wacv21_sfda_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;vinod_sfda&#39;)" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/2102.09003">arXiv</a> |
      <a href="https://delta-lab-iitk.github.io/SFDA/">code</a> |

      <p align="justify"> <i id="wacv21_sfda_abs" style="display: none;">Unsupervised Domain adaptation methods solve the adaptation problem for an unlabeled target set, assuming that the source dataset is available with all labels. However, the availability of actual source samples is not always possible in practical cases. It could be due to memory constraints, privacy concerns, and challenges in sharing data. This practical scenario creates a bottleneck in the domain adaptation problem. This paper addresses this challenging scenario by proposing a domain adaptation technique that does not need any source data. Instead of the source data, we are only provided with a classifier that is trained on the source data. Our proposed approach is based on a generative framework, where the trained classifier is used for generating samples from the source classes. We learn the joint distribution of data by using the energy-based modeling of the trained classifier. At the same time, a new classifier is also adapted for the target domain. We perform various ablation analysis under different experimental setups and demonstrate that the proposed approach achieves better results than the baseline models in this extremely novel scenario.</i></p>

 <pre xml:space="preserve" style="display: none;">@inproceedings{kurmi2021_sfda,
Author = {Kurmi, Vinod Kumar and K S ,
Venaktesh and  Namboodiri, Vinay},
Title = {Domain Impression: A Source Data Free
Domain Adaptation Method},
Booktitle = {WACV},
Year   = {2021}
}
</pre>
      </div>
    </td>
  </tr>


  <tr>
    <td width="33%" valign="top" align="center" style="padding-top:10px"><a href="https://delta-lab-iitk.github.io/Incremental-learning-AU/"><img src="./vinod_files/wacv2021_incre.png" alt="sym" width="100%" style="padding:10px;border-radius:15px;border:0px solid black"></a></td>
        <td width="67%" valign="top" style="padding-top:10px">
      <p><a href="https://delta-lab-iitk.github.io/Incremental-learning-AU/" id="WACV21_2">
<!--          <img src="./vinod_files/new.png"  width="6%" style="border-style: none"> -->
  <heading>Do not Forget to Attend to Uncertainty while
Mitigating Catastrophic Forgetting</heading></a><br>
      <strong>Vinod K Kurmi</strong>, Badri Narayana Patro, Venkatesh K Subramanian,  Vinay P Namboodiri<br>
       <em>IEEE Winter Conference of Applications on Computer Vision (WACV)</em>, 2021.
      </p>

      <div class="paper" id="vinod_incre">
      <a href="https://delta-lab-iitk.github.io/Incremental-learning-AU/">webpage</a> |
      <a href="https://openaccess.thecvf.com/content/WACV2021/html/Kurmi_Do_Not_Forget_to_Attend_to_Uncertainty_While_Mitigating_Catastrophic_WACV_2021_paper.html">pdf</a> |
      <a href="javascript:toggleblock(&#39;wacv21_incre_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;vinod_incre&#39;)" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/2102.01906">arXiv</a> |
      <a href="https://delta-lab-iitk.github.io/Incremental-learning-AU/">code</a> |

      <p align="justify"> <i id="wacv21_incre_abs" style="display: none;">One of the major limitations of deep learning models is that they face catastrophic forgetting in an incremental learning scenario.  There have been several approaches proposed to tackle the problem of incremental learning. Most of these methods are based on knowledge distillation and do not adequately utilize the information provided by older task models, such as uncertainty estimation in predictions. The predictive uncertainty provides the distributional information can be applied to mitigate catastrophic forgetting in a deep learning framework.  In the proposed work, we consider a Bayesian formulation to obtain the data and model uncertainties. We also incorporate self-attention framework to address the incremental learning problem. We define distillation losses in terms of aleatoric uncertainty and self-attention. In the proposed work, we investigate different ablation analyses on these losses. Furthermore, we are able to obtain better results in terms of accuracy on standard benchmarks.</i></p>

 <pre xml:space="preserve" style="display: none;">@inproceedings{kurmi2021_incre,
Author = {Kurmi, Vinod Kumar and Patro, Badri Narayana
and K S , Venaktesh and  Namboodiri, Vinay},
Title = {Do not Forget to Attend to Uncertainty while
Mitigating Catastrophic Forgetting},
Booktitle = {WACV},
Year   = {2021}
}
</pre>
      </div>
    </td>
  </tr>


      <tr>
    <td width="33%" valign="top" align="center" style="padding-top:10px"><a href="https://vinodkkurmi.github.io/"><img src="./vinod_files/w_wacv.png" alt="sym" width="90%" style="padding:10px;border-radius:15px;border:0px solid black"></a></td>
        <td width="67%" valign="top" style="padding-top:10px">
      <p><a href="https://vinodkkurmi.github.io/" id="wacv_w">
  <heading>Explainable Fingerprint ROI Segmentation Using Monte Carlo Dropout</heading></a><br>
        Indu Joshi, Ayush Utkarsh, Riya Kothari, <strong>Vinod K Kurmi</strong>, Antitza Dantcheva, Sumantra Dutta Roy, Prem Kumar Kalra  <br>
       <em>IEEE Winter Conference on Applications of Computer Vision Workshops (WACVW)</em>, 2021.
      </p>

      <div class="paper" id="vinod_w_wacv">
      <a href="https://openaccess.thecvf.com/content/WACV2021W/XAI4B/papers/Joshi_Explainable_Fingerprint_ROI_Segmentation_Using_Monte_Carlo_Dropout_WACVW_2021_paper.pdf">pdf</a> |
      <a href="javascript:toggleblock(&#39;vinod_w_wacv_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;vinod_w_wacv&#39;)" class="togglebib">bibtex</a> |
      <a href="https://openaccess.thecvf.com/content/WACV2021W/XAI4B/papers/Joshi_Explainable_Fingerprint_ROI_Segmentation_Using_Monte_Carlo_Dropout_WACVW_2021_paper.pdf">arXiv</a> |
    <p align="justify"> <i id="vinod_w_wacv_abs" style="display: none;">A fingerprint Region of Interest (ROI) segmentation module is one of the most crucial components in the fingerprint pre-processing pipeline. It separates the foreground fingerprint and background region due to which feature extraction and matching is restricted to ROI instead of entire fingerprint image. However, state-of-the-art segmentation algorithms act like a black box and do not indicate model confidence. In this direction, we propose an explainable fingerprint ROI segmentation model which indicates the pixels on which the model is uncertain. Towards this, we benchmark four state-of-the-art models for semantic segmentation on fingerprint ROI segmentation. Furthermore, we demonstrate the effectiveness of model uncertainty as an attention mechanism to improve the segmentation performance of the best performing model. Experiments on publicly available Fingerprint Verification Challenge (FVC) databases showcase the effectiveness of the proposed model.</i></p>
<!--       <p align="justify"> <i id="icassp21_abs" style="display: none;">Unsupervised Domain adaptation methods solve the adaptation problem for an unlabeled target set, assuming that the source dataset is available with all labels. However, the availability of actual source samples is not always possible in practical cases. It could be due to memory constraints, privacy concerns, and challenges in sharing data. This practical scenario creates a bottleneck in the domain adaptation problem. This paper addresses this challenging scenario by proposing a domain adaptation technique that does not need any source data. Instead of the source data, we are only provided with a classifier that is trained on the source data. Our proposed approach is based on a generative framework, where the trained classifier is used for generating samples from the source classes. We learn the joint distribution of data by using the energy-based modeling of the trained classifier. At the same time, a new classifier is also adapted for the target domain. We perform various ablation analysis under different experimental setups and demonstrate that the proposed approach achieves better results than the baseline models in this extremely novel scenario.</i></p> -->

 <pre xml:space="preserve" style="display: none;">@inproceedings{joshi021_w,
Author={Joshi, Indu and Kothari, Riya
 and Utkarsh, Ayush and Kurmi, Vinod K
 and Dantcheva,Antitza and Roy,
 Sumantra Dutta and Kalra, Prem Kumar},
Title = {Explainable Fingerprint ROI Segmentation
 Using Monte Carlo Dropout},
Booktitle = {IEEE Winter Conference on Applications
of Computer Vision Workshops (WACVW)},
Year   = {2021}
}
</pre>
      </div>
    </td>
  </tr>


<tr>
    <td width="33%" valign="top" align="center"><a href="https://vinodkkurmi.github.io"><img src="./vinod_files/neuro_1.png" alt="sym" width="110%" style="border-style: none"></a>
    </td><td width="67%" valign="top">
      <p><a href="https://vinodkkurmi.github.io" id="neuro2020">
<!--         <img src="./Vinod_files/new.png"  width="6%" style="border-style: none"> -->
      <heading>Revisiting Paraphrase Question Generator using Pairwise
Discriminator</heading></a><br>
       Badri N Patro, Dev Chauhan, <strong>Vinod K Kurmi</strong>,  Vinay P Namboodiri<br>
      <em>Neurocomputing </em>, 2020<br>
     <!--  <em><a href="https://sites.google.com/view/cvpr2018-robotic-vision/home" style="color:black" target="_blank">Deep Learning in Robotics Vision Workshop</a> (CVPR)</em>, 2018 (Oral)<br> -->
      </p>

      <div class="paper" id="neuro_bib">
      <a href="https://arxiv.org/pdf/1912.13149.pdf">pdf</a> |
      <a href="javascript:toggleblock(&#39;neuro_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;neuro_bib&#39;)" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/pdf/1912.13149.pdf">arXiv</a> |
      <a href="https://github.com/dev-chauhan/PQG-pytorch">code</a>
      <br>

      <p align="justify"> <i id="neuro_abs" style="display: none;">In this paper, we propose a method for obtaining sentence-level embeddings. While the problem of securing word-level embeddings is very well studied, we propose a novel method for obtaining sentence-level embeddings. This is obtained by a simple method in the context of solving the paraphrase generation task. If we use a sequential encoder-decoder model for generating paraphrase, we would like the generated paraphrase to be semantically close to the original sentence. One way to ensure this is by adding constraints for true paraphrase embeddings to be close and unrelated paraphrase candidate sentence embeddings to be far. This is ensured by using a sequential pair-wise discriminator that shares weights with the encoder that is trained with a suitable loss function. Our loss function penalizes paraphrase sentence embedding distances from being too large. This loss is used in combination with a sequential encoder-decoder network. We also validated our method by evaluating the obtained embeddings for a sentiment analysis task. The proposed method results in semantic embeddings and outperforms the state-of-the-art on the paraphrase generation and sentiment analysis task on standard datasets. These results are also shown to be statistically significant.</i></p>

<pre xml:space="preserve" style="display: none;">@inproceedings{patro2020Bayesian,
title={Revisiting Paraphrase Question Generator
using Pairwise Discriminator},
author={Patro, Badri Narayana and Chauhan, Dev
and Kurmi, Vinod Kumar and Namboodiri, Vinay},
booktitle={Neurocomputing},
year   = {2020}


<!--   year={2018} -->
}
</pre>
      </div>
    </td>
  </tr>



<tr>
    <td width="33%" valign="top" align="center"><a href="https://vinodkkurmi.github.io"><img src="./vinod_files/wacv2020.png" alt="sym" width="90%"  style="border-style: none"></a>
    </td><td width="67%" valign="top">
      <p><a href="https://vinodkkurmi.github.io" id="wacv2020">
<!--         <img src="./Vinod_files/new.png"  width="6%" style="border-style: none"> -->
      <heading>Deep Bayesian Network for Visual Question Generation</heading></a><br>
       Badri N Patro, <strong>Vinod K Kurmi</strong>, Sandeep Kumar,  Vinay P Namboodiri<br>
      <em>IEEE Winter Conference of Applications on Computer Vision (WACV)</em>, 2020<br>.
     <!--  <em><a href="https://sites.google.com/view/cvpr2018-robotic-vision/home" style="color:black" target="_blank">Deep Learning in Robotics Vision Workshop</a> (CVPR)</em>, 2018 (Oral)<br> -->
      </p>

      <div class="paper" id="wacv2020_bay">
      <a href="https://delta-lab-iitk.github.io/BVQG/">webpage</a> |
      <a href="https://arxiv.org/pdf/2001.08779.pdf">pdf</a> |
      <a href="javascript:toggleblock(&#39;wacv2020_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;wacv2020_bay&#39;)" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/pdf/2001.08779.pdf">arXiv</a> |
      <a href="https://github.com/DelTA-Lab-IITK/BVQG">code</a>
      <br>

      <p align="justify"> <i id="wacv2020_abs" style="display: none;">Generating natural questions from an image is a semantic task that requires using visual and language modality to learn multimodal representations. Images can have multiple visual and language contexts that are relevant for generating questions namely places, captions, and tags. In this paper, we propose the use of exemplars for obtaining the relevant context. We obtain this by using a Multimodal Differential Network to produce natural and engaging questions. The generated questions show a remarkable similarity to the natural questions as validated by a human study. Further, we observe that the proposed approach substantially improves over state-of-the-art benchmarks on the quantitative metrics (BLEU, METEOR, ROUGE, and CIDEr).</i></p>

<pre xml:space="preserve" style="display: none;">@inproceedings{patro2020Bayesian,
  title={Deep Bayesian Network for Visual Question Generation},
  author={Patro, Badri Narayana and Kumar, Sandeep
  and Kurmi, Vinod Kumar and Namboodiri, Vinay},
  booktitle={IEEE Winter Conference of Applications
  on Computer Vision (WACV)},
  year   = {2020}
}
</pre>
      </div>
    </td>
  </tr>




  <tr>
    <td width="33%" valign="top" align="center"><a href="https://delta-lab-iitk.github.io/CD3A/"><img src="./vinod_files/bmvc19.png" alt="sym" width="100%" style="border-radius:15px"></a>
    </td><td width="67%" valign="top">
      <p><a href="https://delta-lab-iitk.github.io/CD3A/" id="BMVC">
<!--       <img src="./Vinod_files/new.png"  width="6%" style="border-style: none"> -->
      <heading>Curriculum based Dropout Discriminator for Domain Adaptation</heading></a><br>
      <strong>Vinod K Kurmi</strong>, Vipul Bajaj, Venkatesh K Subramanian, Vinay P Namboodiri<br>
      <em>British Machine Vision Conference (BMVC) </em>, 2019<br>
      </p>

      <div class="paper" id="bmvc19">
      <a href="https://delta-lab-iitk.github.io/CD3A/">webpage</a> |
      <a href="https://arxiv.org/pdf/1907.10628.pdf">pdf</a> |
      <a href="javascript:toggleblock(&#39;bmvc19_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;bmvc19&#39;)" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/pdf/1907.10628.pdf">arXiv</a> |
      <a href="https://github.com/DelTA-Lab-IITK/CD3A">code</a>
     <!--  <a href="https://youtu.be/POlrWt32_ec">video</a> |
      <a href="https://videoken.com/embed/D0UmVbbJxS8?tocitem=89">oral talk</a> -->
      <br>

      <p align="justify"> <i id="bmvc19_abs" style="display: none;">Domain adaptation is essential to enable wide usage of deep learning based networkstrained using large labeled datasets. Adversarial learning based techniques have showntheir utility towards solving this problem using a discriminator that ensures source andtarget distributions are close. However, here we suggest that rather than using a pointestimate, it would be useful if a distribution based discriminator could be used to bridgethis gap. This could be achieved using multiple classifiers or using traditional ensemblemethods. In contrast, we suggest that a Monte Carlo dropout based ensemble discrim-inator could suffice to obtain the distribution based discriminator. Specifically, we pro-pose a curriculum based dropout discriminator that gradually increases the variance ofthe sample based distribution and the corresponding reverse gradients are used to alignthe source and target feature representations. The detailed results and thorough ablationanalysis show that our model outperforms state-of-the-art results.</i></p>

<pre xml:space="preserve" style="display: none;">@article{kurmi2019curriculum,
title={Curriculum based Dropout Discriminator for Domain Adaptation},
author={Kurmi, Vinod Kumar and Bajaj, Vipul
and Subramanian, Venkatesh K and Namboodiri, Vinay P},
journal={BMVC},
year={2019} }
</pre>
      </div>
    </td>
  </tr>



  <tr>
    <td width="33%" valign="top" align="center"><a href="https://vinodkkurmi.github.io/DiscriminatorDomainAdaptation/"><img src="./vinod_files/ijcnn.png" alt="sym" width="100%" style="border-radius:15px"></a>
    </td><td width="67%" valign="top">
      <p><a href="https://vinodkkurmi.github.io/DiscriminatorDomainAdaptation/" id="IJCNN2019">
<!--       <img src="./Vinod_files/new.png"  width="6%" style="border-style: none"> -->
      <heading>Looking back at Labels: A Class based Domain Adaptation Technique</a><br>
      <strong>Vinod K Kurmi</strong>, Vinay P Namboodiri<br>
      <em>International Joint Conference on Neural Networks (IJCNN)</em>, 2019.
      <strong style="color:black">(Oral presentation)</strong>
      </p>

      <div class="paper" id="ijcnn19_bib">
      <a href="https://vinodkkurmi.github.io/DiscriminatorDomainAdaptation/">webpage</a> |
      <a href="https://arxiv.org/pdf/1904.01341.pdf">pdf</a> |
      <a href="javascript:toggleblock(&#39;ijcnn19_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;ijcnn19_bib&#39;)" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/pdf/1904.01341.pdf">arXiv</a> |
      <a href="http://home.iitk.ac.in/~vinodkk/idda_model/idda_ppt">ppt</a> |
      <a href="https://github.com/vinodkkurmi/DiscriminatorDomainAdaptation">code</a>
      <br>

      <p align="justify"> <i id="ijcnn19_abs" style="display: none;">In this paper, we tackle a problem of Domain Adaptation. In a domain adaptation setting, there is provided a labeled set of examples in a source dataset with multiple classes being present and a target dataset that has no supervision. In this setting, we propose an adversarial discriminator based approach. While the approach based on adversarial discriminator has been previously proposed; in this paper, we present an informed adversarial discriminator. Our observation relies on the analysis that shows that if the discriminator has access to all the information available including the class structure present in the source dataset, then it can guide the transformation of features of the target set of classes to a more structured adapted space. Using this formulation, we obtain the state-of-the-art results for the standard evaluation on benchmark datasets. We further provide detailed analysis which shows that using all the labeled information results in an improved domain adaptation.</i></p>

<pre xml:space="preserve" style="display: none;">@InProceedings{kurmi2019looking,
author = {Kurmi, Vinod Kumar and
Namboodiri, Vinay P},
title = {Looking back at Labels:
A Class based Domain Adaptation
Technique},
booktitle = {International Joint
Conference on Neural Networks (IJCNN) },
month = {July},
year = {2019}
}
</pre>
      </div>
    </td>
  </tr>



<tr>
    <td width="33%" valign="top" align="center"><a href="https://delta-lab-iitk.github.io/CADA/"><img src="./vinod_files/cvpr19.png" alt="sym" width="100%" style="border-radius:15px"></a>
    </td><td width="67%" valign="top">
      <p><a href="https://delta-lab-iitk.github.io/CADA/" id="CVPR19">
<!--       <img src="./Vinod_files/new.png"  width="6%" style="border-style: none"> -->
      <heading>Attending to Discriminative Certainty for Domain Adaptation</heading></a><br>
       <strong>Vinod K Kurmi*</strong>, Shanu Kumar*, Vinay P. Namboodiri&nbsp;(*equal contributions)<br>
      <em>Conference on Computer Vision and Pattern Recognition (CVPR) </em>, 2019.<br>
      </p>

      <div class="paper" id="cada_bib">
      <a href="https://delta-lab-iitk.github.io/CADA/">webpage</a> |
      <a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Kurmi_Attending_to_Discriminative_Certainty_for_Domain_Adaptation_CVPR_2019_paper.html">pdf</a> |
      <a href="javascript:toggleblock(&#39;cada_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;cada_bib&#39;)" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/1906.03502">arXiv</a> |
      <a href="http://home.iitk.ac.in/~vinodkk/cada/CADA_CVPR2019.pdf">poster</a>|
      <a href="https://github.com/DelTA-Lab-IITK/CADA">code</a>|
      <a href="http://home.iitk.ac.in/~vinodkk/cada/Supplementary_cada.pdf">supplementary</a>
      <a href="https://pathak22.github.io/large-scale-curiosity/index.html#sourceCode">code</a>

      <p align="justify"> <i id="cada_abs" style="display: none;">n this paper, we aim to solve for unsupervised domain adaptation of classifiers where we have access to label information for the source domain while these are not available for a target domain. While various methods have been proposed for solving these including adversarial discriminator based methods, most approaches have focused on the entire image based domain adaptation. In an image, there would be regions that can be adapted better, for instance, the foreground object may be similar in nature. To obtain such regions, we propose methods that consider the probabilistic certainty estimate of various regions and specific focus on these during classification for adaptation. We observe that just by incorporating the probabilistic certainty of the discriminator while training the classifier, we are able to obtain state of the art results on various datasets as compared against all the recent methods. We provide a thorough empirical analysis of the method by providing ablation analysis, statistical significance test, and visualization of the attention maps and t-SNE embeddings. These evaluations convincingly demonstrate the effectiveness of the proposed approach.</i></p>

<pre xml:space="preserve" style="display: none;">@InProceedings{Kurmi_2019_CVPR,
author = {Kumar Kurmi, Vinod and Kumar, Shanu
and Namboodiri, Vinay P.},
title = {Attending to Discriminative Certainty
for Domain Adaptation},
booktitle = {IEEE Computer Society Conference
on Computer Vision and Pattern Recognition(CVPR),},
month = {June},
year = {2019}
}
</pre>
      </div>
    </td>
  </tr>



<tr>
    <td width="33%" valign="top" align="center"><a href="https://badripatro.github.io/MDN-VQG/"><img src="./vinod_files/emnlp18.png" alt="sym" width="100%" style="border-style: none"></a>
    </td><td width="67%" valign="top">
      <p><a href="https://badripatro.github.io/MDN-VQG/" id="MDN-VQG">
      <heading>Multimodal Differential Network for Visual Question Generation</heading></a><br>
       Badri N Patro, Sandeep Kumar, <strong>Vinod K Kurmi</strong>, Vinay P Namboodiri<br>
      <em>Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 2018.<br>
     <!--  <em><a href="https://sites.google.com/view/cvpr2018-robotic-vision/home" style="color:black" target="_blank">Deep Learning in Robotics Vision Workshop</a> (CVPR)</em>, 2018 (Oral)<br> -->
      </p>

      <div class="paper" id="MDN_VQG_bib">
      <a href="https://badripatro.github.io/MDN-VQG/">webpage</a> |
      <a href="https://arxiv.org/pdf/1808.03986.pdf">pdf</a> |
      <a href="javascript:toggleblock(&#39;MDN_VQG_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;MDN_VQG_bib&#39;)" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/pdf/1808.03986.pdf">arXiv</a> |
      <a href="https://github.com/vinodkkurmi/MDN-VQG/">code</a>
      <br>

      <p align="justify"> <i id="MDN_VQG_abs" style="display: none;">Generating natural questions from an image is a semantic task that requires using visual and language modality to learn multimodal representations. Images can have multiple visual and language contexts that are relevant for generating questions namely places, captions, and tags. In this paper, we propose the use of exemplars for obtaining the relevant context. We obtain this by using a Multimodal Differential Network to produce natural and engaging questions. The generated questions show a remarkable similarity to the natural questions as validated by a human study. Further, we observe that the proposed approach substantially improves over state-of-the-art benchmarks on the quantitative metrics (BLEU, METEOR, ROUGE, and CIDEr).</i></p>

<pre xml:space="preserve" style="display: none;">@inproceedings{patro2018multimodal,
  title={Multimodal Differential Network
  for Visual Question Generation},
  author={Patro, Badri Narayana and Kumar,
  Sandeep and Kurmi, Vinod Kumar and Namboodiri, Vinay},
  booktitle={Proceedings of the 2018 Conference
  on Empirical Methods in Natural Language Processing},
  pages={4002--4012},
  year={2018}
}
</pre>
      </div>
    </td>
  </tr>




 <tr>
    <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/1807.07560"><img src="./vinod_files/coling18.png" alt="sym" width="100%" style="border-radius:15px"></a>
    </td><td width="67%" valign="top">
      <p><a href="https://arxiv.org/abs/1807.07560" id="COLING2019">
      <heading>Learning Semantic Sentence Embeddings using Sequential Pair-wise Discriminator</heading></a><br>
      Badri N Patro*,<strong>Vinod K Kurmi*</strong>, Sandeep K*, Vinay P Namboodiri&nbsp;(*equal contributions)<br>
      <em>International Conference on Computational Linguistics (COLING)</em>, 2018.<br>
      </p>

      <div class="paper" id="coling18">
     <a href="https://github.com/vinodkkurmi/PQG/">webpage</a> |
      <a href="https://www.aclweb.org/anthology/C18-1230">pdf</a> |
      <a href="javascript:toggleblock(&#39;coling18_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;coling18&#39;)" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/1806.00807">arXiv</a> |
      <a href="https://github.com/vinodkkurmi/PQG/">code</a>
      <!-- <a href="https://pathak22.github.io/large-scale-curiosity/index.html#sourceCode">code</a> -->
      <br>

      <p align="justify"> <i id="coling18_abs" style="display: none;">In this paper, we propose a method for obtaining sentence-level embeddings. While the problem of securing word-level embeddings is very well studied, we propose a novel method for obtaining sentence-level embeddings. This is obtained by a simple method in the context of solving the paraphrase generation task. If we use a sequential encoder-decoder model for generating paraphrase, we would like the generated paraphrase to be semantically close to the original sentence. One way to ensure this is by adding constraints for true paraphrase embeddings to be close and unrelated paraphrase candidate sentence embeddings to be far. This is ensured by using a sequential pair-wise discriminator that shares weights with the encoder that is trained with a suitable loss function. Our loss function penalizes paraphrase sentence embedding distances from being too large. This loss is used in combination with a sequential encoder-decoder network. We also validated our method by evaluating the obtained embeddings for a sentiment analysis task. The proposed method results in semantic embeddings and outperforms the state-of-the-art on the paraphrase generation and sentiment analysis task on standard datasets. These results are also shown to be statistically significant.</i></p>

<pre xml:space="preserve" style="display: none;">@inproceedings{patro2018learning,
  title={Learning Semantic Sentence Embeddings
  using Sequential Pair-wise Discriminator},
  author={Patro, Badri Narayana and Kurmi,
  Vinod Kumar and Kumar, Sandeep and
  Namboodiri, Vinay},
  booktitle={Proceedings of the 27th
  International Conference on Computational
  Linguistics},
  pages={2715--2729},
  year={2018}
}
</pre>
      </div>
    </td>
  </tr>



<tr>
    <td width="33%" valign="top" align="center"><a href="http://home.iitk.ac.in/~vinodkk/pdf_files/gesture_paper"><img src="./vinod_files/gesture2.png" alt="sym" width="100%" style="border-radius:15px"></a>
    </td>
    <td width="67%" valign="top">
      <p><a href="http://home.iitk.ac.in/~vinodkk/pdf_files/gesture_paper" id="WSCG">
      <heading>Robust hand gesture recognition from 3D data</heading></a><br>
      <strong>Vinod K Kurmi</strong>, Garima Jain, Venkatesh K Subramanian <br>
      <em>International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision (WSCG)</em>, 2015.<br>
      <!-- <strong style="color:red">Oral presentation <a href="https://vimeo.com/237270588" target="_blank" style="color:red">[video]</a></strong> -->
      <!-- <strong style="color:black">(Oral presentation)</strong> -->
      </p>

      <div class="paper" id="wscg_bib">
      <a href="http://home.iitk.ac.in/~vinodkk/pdf_files/gesture_paper">pdf</a> |
      <!-- <a href="http://pathak22.github.io/zeroshot-imitation/resources/iclr18.pdf">pdf</a> | -->
      <a href="javascript:toggleblock(&#39;wscg_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;wscg_bib&#39;)" class="togglebib">bibtex</a> |
      <a href="http://wscg.zcu.cz/WSCG2015/!_2015_WSCG_SHORT_proceedings.pdf" target="_blank">weblink</a> |
      <a href="https://github.com/vinodkkurmi/">code</a> |
      <a href="http://www.iitk.ac.in/siic/d/content/human-hand-detection-system-apparatus-and-method-thereof">webpage</a> |
      <a href="https://www.youtube.com/watch?v=ejyaQ4N2waI&feature=youtu.be">video</a>

      <br>

      <p align="justify"> <i id="wscg_abs" style="display: none;">In this paper, we use the output of a 3D sensor (ex. Kinect from Microsoft) to capture depth images of humans making a set of predefined hand gestures in various body poses. Conventional approaches using Kinect data have been constrained by the limitation of the human detector middleware that requires close conformity to a standard near erect, legs apart, hands apart pose for the subject. Our approach also permits clutter and possible motion in the scene background, and to a limited extent, in the foreground as well. We make an important point in this work to emphasize that the recognition performance is considerably improved by a choice of hand gestures that accommodate the sensor’s specific limitations. These sensor limitations include low resolution in x and y as well as z. Hand gestures have been chosen (designed) for easy detection by seeking to detect a fingers apart, fingertip constellation with minimum computation. without, however compromising on issues of utility or ergonomy. It is shown that these gestures can be recognised in real time irrespective of visible band illumination levels, background motion, foreground clutter, user body pose, gesturing speeds and user distance. The last is of course limited by the sensor’s own range limitations. Our main contributions are the selection and design of gestures suitable for limited range, limited resolution 3D sensors and the novel method of depth slicing used to extract hand features from the background. This obviates the need for preliminary human detection and enables easy detection and highly reliable and fast (30 fps) gesture classification.</i></p>

<pre xml:space="preserve" style="display: none;">@article{kurmi2015robust,
  title={Robust hand gesture recognition
  from 3D data},
  author={Kurmi, Vinod K and Jain, Garima
  and Venkatesh, KS},
  year={2015},
  publisher={V{\'a}clav Skala-UNION Agency}
}
</pre>

    </div>
    </td>
  </tr>




 <tr>
    <!-- <td width="33%" valign="top" align="center"><a href="http://home.iitk.ac.in/~vinodkk/pdf_files/gesture_paper"><img src="./Vinod_files/depth.png" alt="sym" width="65%" border="1" style="border-color:black"><hr style="height:0pt; visibility:hidden; margin:0"><img src="./Deepak Pathak_files/iclr18_2.gif" alt="sym" width="65%" border="1" style="border-color:black"></a> -->
 <!--    </td> -->
    <td width="33%" valign="top" align="center"><a href="http://home.iitk.ac.in/~vinodkk/pdf_files/gesture_paper"><img src="./vinod_files/depth.png" alt="sym" width="100%" style="border-radius:15px"></a>
    </td>
    <td width="67%" valign="top">
      <p><a href="http://home.iitk.ac.in/~vinodkk/pdf_files/gesture_paper" id="patent">
      <heading>A Human-Hand Detection System, Apparatus and A Method Thereof</a><br>
      <strong>Vinod K Kurmi</strong>, Garima Jain, Venkatesh K Subramanian <br>
      <em>Indian Patent</em>, 2016.<br>
      <!-- <strong style="color:red">Oral presentation <a href="https://vimeo.com/237270588" target="_blank" style="color:red">[video]</a></strong> -->
      <!-- <strong style="color:black">(Oral presentation)</strong> -->
      </p>

      <div class="paper" id="iclr18">
      <!-- <a href="http://pathak22.github.io/zeroshot-imitation/resources/iclr18.pdf">pdf</a> | -->
<!--       <a href="javascript:toggleblock(&#39;iclr18_abs&#39;)">abstract</a> |
      <a shape="rect" href="javascript:togglebib(&#39;iclr18&#39;)" class="togglebib">bibtex</a> | -->
       <a href="https://iprsearch.ipindia.gov.in/RQStatus/PatentCertificatePDF.aspx?AppNo=MTY3OC9ERUwvMjAxNQ==&FullPath=LVBhdGVudENlcnRpZmljYXRlMTQtMDMtMjAyNC5wZGY=">Certificate</a> |
      <a href="http://www.ipindia.nic.in/writereaddata/Portal/IPOJournal/1_419_1/Part1.pdf" target="_blank">weblink</a> |
      <a href="https://www.youtube.com/watch?v=ejyaQ4N2waI&feature=youtu.be">video</a>

      <br>
<!--
      <p align="justify"> <i id="iclr18_abs" style="display: none;">In this paper, we use the output of a 3D sensor (ex. Kinect from Microsoft) to capture depth images of humans making a set of predefined hand gestures in various body poses. Conventional approaches using Kinect data have been constrained by the limitation of the human detector middleware that requires close conformity to a standard near erect, legs apart, hands apart pose for the subject. Our approach also permits clutter and possible motion in the scene background, and to a limited extent, in the foreground as well. We make an important point in this work to emphasize that the recognition performance is considerably improved by a choice of hand gestures that accommodate the sensor’s specific limitations. These sensor limitations include low resolution in x and y as well as z. Hand gestures have been chosen (designed) for easy detection by seeking to detect a fingers apart, fingertip constellation with minimum computation. without, however compromising on issues of utility or ergonomy. It is shown that these gestures can be recognised in real time irrespective of visible band illumination levels, background motion, foreground clutter, user body pose, gesturing speeds and user distance. The last is of course limited by the sensor’s own range limitations. Our main contributions are the selection and design of gestures suitable for limited range, limited resolution 3D sensors and the novel method of depth slicing used to extract hand features from the background. This obviates the need for preliminary human detection and enables easy detection and highly reliable and fast (30 fps) gesture classification.</i></p>
 -->
<!-- <pre xml:space="preserve" style="display: none;">@article{kurmi2015robust,
  title={Robust hand gesture recognition from 3D data},
  author={Kurmi, Vinod K and Jain, Garima and Venkatesh, KS},
  year={2015},
  publisher={V{\'a}clav Skala-UNION Agency}
}
</pre> -->

   </div>
    </td>
  </tr>





</tbody></table>


<hr>

<table width="100%" align="center" border="0" cellpadding="10">
  <tbody><tr><td>
    <sectionheading>&nbsp;&nbsp;Selected Awards</sectionheading>
    <ul>
        <li>   <strong>  Outstanding PhD Thesis Award </strong> (2021).</li>
    <li>   <strong>  <a href="https://www.daad.de/en/the-daad/postdocnet/fellows/fellows/#Kurmi
"> AI Post-Doc Net Fellow</a> </strong> (2020-2021).</li>
    <li> <strong>TCS Research Fellowship </strong> (2016-2019).</li>
    <li>  <strong>Qualcomm Innovation Fellowship </strong> 2019 Finalists (2019).</li>
    <li> MHRD PhD Fellowship (2015-16).</li>
    <li> Qualified for <strong>Junior Research Fellowship(JRF)</strong> of CSIR-NET(2014).</li>
    <li> MHRD Post-Graduate Fellowship (2012-14).</li>
    </ul>
  </td></tr>
</tbody></table>

<hr>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="2">
    <tbody><tr><td><br><p align="right"><font size="1.5">
    Modified version of template from <a href="http://www.cs.berkeley.edu/~barron/">this</a>, <a href="https://www.cs.cmu.edu/~dpathak/">this,</a> and <a href="http://jeffdonahue.com/">this</a>
    </font></p></td></tr>
</tbody></table>



  </td></tr>
</tbody></table>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>

<script xml:space="preserve" language="JavaScript">
  hideblock('interspeech25_abs');
  </script>
<script xml:space="preserve" language="JavaScript">
hideblock('wacv25inc_abs');
</script>

<script xml:space="preserve" language="JavaScript">
hideblock('wacv25ss_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('wacv25sfda_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('interspeech24_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('cikm24_abs');
</script>

<script xml:space="preserve" language="JavaScript">
hideblock('bmvc24_abs');
</script>

<script xml:space="preserve" language="JavaScript">
hideblock('ivc3_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('interspeech22_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('aaai22_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('wacv22_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('ivc_2_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('shanu_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('blessen_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('vinod_cd3a_j_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('vinod_idda_j_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('ijcnn21_1_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('ijcnn21_2_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('icassp21_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('wacv21_sfda_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('wacv21_incre_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('vinod_w_wacv_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('neuro_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('wacv2020_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('bmvc19_abs');
</script>

<script xml:space="preserve" language="JavaScript">
hideblock('ijcnn19_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('cada_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('MDN_VQG_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('coling18_abs');
</script>

<script xml:space="preserve" language="JavaScript">
hideblock('wscg_abs');
</script>


</body></html>
